INFO 07-15 14:53:13 [__init__.py:244] Automatically detected platform cuda.
INFO 07-15 14:53:44 [config.py:841] This model supports multiple tasks: {'classify', 'embed', 'generate', 'reward'}. Defaulting to 'generate'.
WARNING 07-15 14:53:44 [config.py:3371] Casting torch.bfloat16 to torch.float16.
INFO 07-15 14:53:44 [config.py:1472] Using max model len 131072
INFO 07-15 14:53:49 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 07-15 14:53:52 [core.py:526] Waiting for init message from front-end.
INFO 07-15 14:53:52 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
INFO 07-15 14:53:59 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 07-15 14:54:00 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 07-15 14:54:00 [gpu_model_runner.py:1770] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 07-15 14:54:00 [gpu_model_runner.py:1775] Loading model from scratch...
INFO 07-15 14:54:01 [cuda.py:284] Using Flash Attention backend on V1 engine.
INFO 07-15 14:54:06 [weight_utils.py:292] Using model weights format ['*.safetensors']
INFO 07-15 14:55:06 [default_loader.py:272] Loading weights took 60.07 seconds
INFO 07-15 14:55:07 [gpu_model_runner.py:1801] Model loading took 14.9889 GiB and 66.133508 seconds
INFO 07-15 14:55:12 [gpu_worker.py:232] Available KV cache memory: 23.66 GiB
INFO 07-15 14:55:13 [kv_cache_utils.py:716] GPU KV cache size: 193,856 tokens
INFO 07-15 14:55:13 [kv_cache_utils.py:720] Maximum concurrency for 131,072 tokens per request: 1.48x
INFO 07-15 14:55:19 [gpu_model_runner.py:2326] Graph capturing finished in 6 secs, took 0.15 GiB
INFO 07-15 14:55:19 [core.py:172] init engine (profile, create kv cache, warmup model) took 12.38 seconds
Processing batch starting at item 0 (1000 items remaining)
Processing batch starting at item 256 (744 items remaining)
Processing batch starting at item 512 (488 items remaining)
Processing batch starting at item 768 (232 items remaining)

Generation complete!
Generated 1000 preference pairs for user: user6 (test split)
Dataset saved to: ../../data/preference/user6_test.json
