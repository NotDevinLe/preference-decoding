log_prob batches:   0%|          | 0/63 [00:00<?, ?it/s]log_prob batches:   2%|▏         | 1/63 [00:04<04:26,  4.30s/it]log_prob batches:   3%|▎         | 2/63 [00:04<01:59,  1.97s/it]log_prob batches:   5%|▍         | 3/63 [00:04<01:13,  1.23s/it]log_prob batches:   6%|▋         | 4/63 [00:05<00:53,  1.10it/s]log_prob batches:   8%|▊         | 5/63 [00:05<00:37,  1.53it/s]log_prob batches:  10%|▉         | 6/63 [00:05<00:29,  1.93it/s]log_prob batches:  11%|█         | 7/63 [00:06<00:35,  1.58it/s]log_prob batches:  13%|█▎        | 8/63 [00:06<00:28,  1.96it/s]log_prob batches:  14%|█▍        | 9/63 [00:07<00:22,  2.39it/s]log_prob batches:  16%|█▌        | 10/63 [00:07<00:19,  2.66it/s]log_prob batches:  17%|█▋        | 11/63 [00:07<00:17,  2.98it/s]log_prob batches:  19%|█▉        | 12/63 [00:08<00:17,  2.94it/s]log_prob batches:  21%|██        | 13/63 [00:08<00:14,  3.34it/s]log_prob batches:  22%|██▏       | 14/63 [00:08<00:15,  3.19it/s]log_prob batches:  24%|██▍       | 15/63 [00:09<00:17,  2.69it/s]log_prob batches:  25%|██▌       | 16/63 [00:09<00:14,  3.16it/s]log_prob batches:  27%|██▋       | 17/63 [00:09<00:12,  3.56it/s]log_prob batches:  29%|██▊       | 18/63 [00:09<00:14,  3.21it/s]log_prob batches:  30%|███       | 19/63 [00:10<00:13,  3.27it/s]log_prob batches:  32%|███▏      | 20/63 [00:10<00:12,  3.31it/s]log_prob batches:  33%|███▎      | 21/63 [00:10<00:13,  3.07it/s]log_prob batches:  35%|███▍      | 22/63 [00:11<00:11,  3.50it/s]log_prob batches:  37%|███▋      | 23/63 [00:11<00:13,  2.90it/s]log_prob batches:  38%|███▊      | 24/63 [00:11<00:12,  3.06it/s]log_prob batches:  40%|███▉      | 25/63 [00:12<00:13,  2.89it/s]log_prob batches:  41%|████▏     | 26/63 [00:12<00:11,  3.18it/s]log_prob batches:  43%|████▎     | 27/63 [00:12<00:10,  3.55it/s]log_prob batches:  44%|████▍     | 28/63 [00:13<00:12,  2.87it/s]log_prob batches:  46%|████▌     | 29/63 [00:13<00:12,  2.64it/s]log_prob batches:  48%|████▊     | 30/63 [00:13<00:11,  2.80it/s]log_prob batches:  49%|████▉     | 31/63 [00:14<00:11,  2.75it/s]log_prob batches:  51%|█████     | 32/63 [00:14<00:12,  2.48it/s]log_prob batches:  52%|█████▏    | 33/63 [00:15<00:13,  2.23it/s]log_prob batches:  54%|█████▍    | 34/63 [00:15<00:11,  2.58it/s]log_prob batches:  56%|█████▌    | 35/63 [00:15<00:09,  2.92it/s]log_prob batches:  57%|█████▋    | 36/63 [00:16<00:08,  3.21it/s]log_prob batches:  59%|█████▊    | 37/63 [00:17<00:16,  1.62it/s]log_prob batches:  60%|██████    | 38/63 [00:17<00:12,  2.02it/s]log_prob batches:  62%|██████▏   | 39/63 [00:17<00:09,  2.46it/s]log_prob batches:  63%|██████▎   | 40/63 [00:18<00:08,  2.66it/s]log_prob batches:  65%|██████▌   | 41/63 [00:18<00:07,  2.76it/s]log_prob batches:  67%|██████▋   | 42/63 [00:18<00:07,  2.99it/s]log_prob batches:  68%|██████▊   | 43/63 [00:18<00:05,  3.36it/s]log_prob batches:  70%|██████▉   | 44/63 [00:19<00:05,  3.70it/s]log_prob batches:  71%|███████▏  | 45/63 [00:19<00:04,  3.75it/s]log_prob batches:  71%|███████▏  | 45/63 [00:23<00:09,  1.95it/s]
Traceback (most recent call last):
  File "/mmfs1/gscratch/ark/devinl6/preference/preference-decoding/utils/eval_approx.py", line 63, in <module>
    accuracy = get_approximation_accuracy(
  File "/mmfs1/gscratch/ark/devinl6/preference/preference-decoding/utils/drift.py", line 354, in get_approximation_accuracy
    yw_base_probs, _ = log_prob(model_ds, yw_list, [base_prompt] * n, questions, device, tokenizer, batch_size=batch_size)
  File "/mmfs1/gscratch/ark/devinl6/preference/preference-decoding/utils/drift.py", line 61, in log_prob
    probs = torch.nn.functional.log_softmax(outputs, dim=-1)
  File "/gscratch/ark/devinl6/envs/align/lib/python3.10/site-packages/torch/nn/functional.py", line 2248, in log_softmax
    ret = input.log_softmax(dim)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 22.66 GiB. GPU 0 has a total capacity of 44.42 GiB of which 13.81 GiB is free. Including non-PyTorch memory, this process has 30.60 GiB memory in use. Of the allocated memory 27.69 GiB is allocated by PyTorch, and 2.41 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
