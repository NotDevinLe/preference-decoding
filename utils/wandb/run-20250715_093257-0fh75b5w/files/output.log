[INFO|integration_utils.py:880] 2025-07-15 09:32:59,159 >> Could not log the number of model parameters in Weights & Biases due to an AttributeError.
 11%|███████▍                                                              | 100/939 [27:55<3:30:12, 15.03s/it][INFO|trainer.py:4327] 2025-07-15 10:00:54,238 >>
{'loss': 0.7568, 'grad_norm': 9.5598783493042, 'learning_rate': 9.574468085106384e-07, 'epoch': 0.03}
{'loss': 0.7823, 'grad_norm': 7.927206516265869, 'learning_rate': 2.021276595744681e-06, 'epoch': 0.06}
{'loss': 0.7397, 'grad_norm': 8.703330039978027, 'learning_rate': 3.0851063829787237e-06, 'epoch': 0.1}
{'loss': 0.7013, 'grad_norm': 10.386847496032715, 'learning_rate': 4.148936170212766e-06, 'epoch': 0.13}
{'loss': 0.7138, 'grad_norm': 9.333751678466797, 'learning_rate': 5.212765957446809e-06, 'epoch': 0.16}
{'loss': 0.6548, 'grad_norm': 7.345139026641846, 'learning_rate': 6.276595744680851e-06, 'epoch': 0.19}
{'loss': 0.5699, 'grad_norm': 7.964768886566162, 'learning_rate': 7.340425531914894e-06, 'epoch': 0.22}
{'loss': 0.4474, 'grad_norm': 7.162090301513672, 'learning_rate': 8.404255319148937e-06, 'epoch': 0.26}
{'loss': 0.417, 'grad_norm': 5.5737409591674805, 'learning_rate': 9.46808510638298e-06, 'epoch': 0.29}
{'loss': 0.3551, 'grad_norm': 5.38552713394165, 'learning_rate': 9.999136119166803e-06, 'epoch': 0.32}
***** Running Evaluation *****
[INFO|trainer.py:4329] 2025-07-15 10:00:54,238 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-15 10:00:54,239 >>   Batch size = 1
 21%|██████████████▉                                                       | 200/939 [59:04<4:14:02, 20.63s/it][INFO|trainer.py:4327] 2025-07-15 10:32:03,377 >>
***** Running Evaluation *****                                                                                 
{'eval_loss': 0.275745689868927, 'eval_accuracy': 0.884, 'eval_runtime': 128.079, 'eval_samples_per_second': 7.808, 'eval_steps_per_second': 7.808, 'epoch': 0.32}
{'loss': 0.2907, 'grad_norm': 5.142919540405273, 'learning_rate': 9.9922268634943e-06, 'epoch': 0.35}
{'loss': 0.1894, 'grad_norm': 5.103487014770508, 'learning_rate': 9.978417901361958e-06, 'epoch': 0.38}
{'loss': 0.1603, 'grad_norm': 4.743302822113037, 'learning_rate': 9.95772831799724e-06, 'epoch': 0.42}
{'loss': 0.1931, 'grad_norm': 4.728066444396973, 'learning_rate': 9.930186708264902e-06, 'epoch': 0.45}
{'loss': 0.1391, 'grad_norm': 4.776456832885742, 'learning_rate': 9.895831137146319e-06, 'epoch': 0.48}
{'loss': 0.1901, 'grad_norm': 4.578513145446777, 'learning_rate': 9.854709087130261e-06, 'epoch': 0.51}
{'loss': 0.1354, 'grad_norm': 3.786046028137207, 'learning_rate': 9.80687739258782e-06, 'epoch': 0.54}
{'loss': 0.1663, 'grad_norm': 1.9484338760375977, 'learning_rate': 9.7524021612222e-06, 'epoch': 0.58}
{'loss': 0.1345, 'grad_norm': 6.437373638153076, 'learning_rate': 9.691358682701927e-06, 'epoch': 0.61}
{'loss': 0.1109, 'grad_norm': 2.8115875720977783, 'learning_rate': 9.623831324603755e-06, 'epoch': 0.64}
[INFO|trainer.py:4329] 2025-07-15 10:32:03,377 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-15 10:32:03,377 >>   Batch size = 1
 32%|█████████████████████▋                                              | 300/939 [1:31:25<4:00:11, 22.55s/it][INFO|trainer.py:4327] 2025-07-15 11:04:24,328 >>
***** Running Evaluation *****                                                                                 
{'eval_loss': 0.10920661687850952, 'eval_accuracy': 0.954, 'eval_runtime': 127.3706, 'eval_samples_per_second': 7.851, 'eval_steps_per_second': 7.851, 'epoch': 0.64}
{'loss': 0.196, 'grad_norm': 4.174684524536133, 'learning_rate': 9.549913415809084e-06, 'epoch': 0.67}
{'loss': 0.1545, 'grad_norm': 4.446645259857178, 'learning_rate': 9.469707117515068e-06, 'epoch': 0.7}
{'loss': 0.1128, 'grad_norm': 0.11989281326532364, 'learning_rate': 9.383323282038632e-06, 'epoch': 0.74}
{'loss': 0.0967, 'grad_norm': 1.8924399614334106, 'learning_rate': 9.29088129960862e-06, 'epoch': 0.77}
{'loss': 0.1486, 'grad_norm': 9.043429374694824, 'learning_rate': 9.192508933357753e-06, 'epoch': 0.8}
{'loss': 0.0802, 'grad_norm': 2.735355854034424, 'learning_rate': 9.088342142742493e-06, 'epoch': 0.83}
{'loss': 0.141, 'grad_norm': 6.00148868560791, 'learning_rate': 8.978524895634842e-06, 'epoch': 0.86}
{'loss': 0.1057, 'grad_norm': 1.5171765089035034, 'learning_rate': 8.86320896934581e-06, 'epoch': 0.9}
{'loss': 0.1432, 'grad_norm': 4.612669944763184, 'learning_rate': 8.742553740855507e-06, 'epoch': 0.93}
{'loss': 0.0812, 'grad_norm': 5.759416580200195, 'learning_rate': 8.616725966539831e-06, 'epoch': 0.96}
[INFO|trainer.py:4329] 2025-07-15 11:04:24,328 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-15 11:04:24,328 >>   Batch size = 1
 43%|████████████████████████████▉                                       | 400/939 [2:02:28<3:05:12, 20.62s/it][INFO|trainer.py:4327] 2025-07-15 11:35:27,884 >>
***** Running Evaluation *****                                                                                 
{'eval_loss': 0.08433414250612259, 'eval_accuracy': 0.957, 'eval_runtime': 127.2791, 'eval_samples_per_second': 7.857, 'eval_steps_per_second': 7.857, 'epoch': 0.96}
{'loss': 0.1066, 'grad_norm': 5.974685192108154, 'learning_rate': 8.485899551698166e-06, 'epoch': 0.99}
{'loss': 0.1107, 'grad_norm': 4.984772205352783, 'learning_rate': 8.350255310200611e-06, 'epoch': 1.02}
{'loss': 0.1485, 'grad_norm': 8.326767921447754, 'learning_rate': 8.209980714586955e-06, 'epoch': 1.05}
{'loss': 0.0691, 'grad_norm': 2.1904549598693848, 'learning_rate': 8.065269636962765e-06, 'epoch': 1.09}
{'loss': 0.093, 'grad_norm': 3.0643444061279297, 'learning_rate': 7.916322081050708e-06, 'epoch': 1.12}
{'loss': 0.0935, 'grad_norm': 10.26358413696289, 'learning_rate': 7.76334390576742e-06, 'epoch': 1.15}
{'loss': 0.0827, 'grad_norm': 9.082371711730957, 'learning_rate': 7.60654654070796e-06, 'epoch': 1.18}
{'loss': 0.0763, 'grad_norm': 2.7003767490386963, 'learning_rate': 7.446146693931111e-06, 'epoch': 1.21}
{'loss': 0.05, 'grad_norm': 3.7662973403930664, 'learning_rate': 7.282366052449351e-06, 'epoch': 1.25}
{'loss': 0.1227, 'grad_norm': 3.126450300216675, 'learning_rate': 7.115430975837457e-06, 'epoch': 1.28}
[INFO|trainer.py:4329] 2025-07-15 11:35:27,885 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-15 11:35:27,885 >>   Batch size = 1
 53%|████████████████████████████████████▏                               | 500/939 [2:34:19<2:10:20, 17.82s/it][INFO|trainer.py:4327] 2025-07-15 12:07:18,808 >>
***** Running Evaluation *****                                                                                 
{'eval_loss': 0.08380550146102905, 'eval_accuracy': 0.955, 'eval_runtime': 127.9321, 'eval_samples_per_second': 7.817, 'eval_steps_per_second': 7.817, 'epoch': 1.28}
{'loss': 0.1273, 'grad_norm': 6.6061506271362305, 'learning_rate': 6.945572183383229e-06, 'epoch': 1.31}
{'loss': 0.0832, 'grad_norm': 1.7632533311843872, 'learning_rate': 6.773024435212678e-06, 'epoch': 1.34}
{'loss': 0.0525, 'grad_norm': 1.5567383766174316, 'learning_rate': 6.598026207830428e-06, 'epoch': 1.37}
{'loss': 0.0518, 'grad_norm': 4.052024841308594, 'learning_rate': 6.4208193645237314e-06, 'epoch': 1.41}
{'loss': 0.0939, 'grad_norm': 6.348706245422363, 'learning_rate': 6.241648821085666e-06, 'epoch': 1.44}
{'loss': 0.1322, 'grad_norm': 7.894546031951904, 'learning_rate': 6.060762207319479e-06, 'epoch': 1.47}
{'loss': 0.1019, 'grad_norm': 2.5505847930908203, 'learning_rate': 5.878409524791931e-06, 'epoch': 1.5}
{'loss': 0.0918, 'grad_norm': 3.0288968086242676, 'learning_rate': 5.694842801308651e-06, 'epoch': 1.53}
{'loss': 0.0971, 'grad_norm': 1.0628330707550049, 'learning_rate': 5.510315742589042e-06, 'epoch': 1.57}
{'loss': 0.0537, 'grad_norm': 6.666146755218506, 'learning_rate': 5.325083381622165e-06, 'epoch': 1.6}
[INFO|trainer.py:4329] 2025-07-15 12:07:18,809 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-15 12:07:18,809 >>   Batch size = 1
 53%|████████████████████████████████████▏                               | 500/939 [2:36:27<2:10:20, 17.82s/it][INFO|trainer.py:3993] 2025-07-15 12:09:26,525 >> Saving model checkpoint to saves/golden/user1/toy_reward/checkpoint-500
[INFO|trainer.py:4007] 2025-07-15 12:09:26,530 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
{'eval_loss': 0.08535429835319519, 'eval_accuracy': 0.954, 'eval_runtime': 127.7159, 'eval_samples_per_second': 7.83, 'eval_steps_per_second': 7.83, 'epoch': 1.6}
[INFO|tokenization_utils_base.py:2356] 2025-07-15 12:09:26,657 >> chat template saved in saves/golden/user1/toy_reward/checkpoint-500/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-15 12:09:26,660 >> tokenizer config file saved in saves/golden/user1/toy_reward/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-15 12:09:26,661 >> Special tokens file saved in saves/golden/user1/toy_reward/checkpoint-500/special_tokens_map.json
[INFO|configuration_utils.py:698] 2025-07-15 12:09:28,146 >> loading configuration file config.json from cache at /mmfs1/gscratch/ark/devinl6/hf_cache/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:770] 2025-07-15 12:09:28,147 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "vocab_size": 128256
}
[INFO|2025-07-15 12:09:28] llamafactory.train.callbacks:143 >> Value head model saved at: saves/golden/user1/toy_reward/checkpoint-500

 64%|███████████████████████████████████████████▍                        | 600/939 [3:04:37<1:33:17, 16.51s/it][INFO|trainer.py:4327] 2025-07-15 12:37:36,452 >>
{'loss': 0.1033, 'grad_norm': 7.930962085723877, 'learning_rate': 5.139401726188208e-06, 'epoch': 1.63}
{'loss': 0.111, 'grad_norm': 3.442491054534912, 'learning_rate': 4.953527405032723e-06, 'epoch': 1.66}
{'loss': 0.1038, 'grad_norm': 2.548131227493286, 'learning_rate': 4.767717313182611e-06, 'epoch': 1.69}
{'loss': 0.0867, 'grad_norm': 2.578754186630249, 'learning_rate': 4.582228256894093e-06, 'epoch': 1.73}
{'loss': 0.0915, 'grad_norm': 2.9189722537994385, 'learning_rate': 4.397316598723385e-06, 'epoch': 1.76}
{'loss': 0.0993, 'grad_norm': 7.66835880279541, 'learning_rate': 4.2132379032105695e-06, 'epoch': 1.79}
{'loss': 0.0972, 'grad_norm': 6.914815902709961, 'learning_rate': 4.030246583666437e-06, 'epoch': 1.82}
{'loss': 0.0487, 'grad_norm': 0.7873724699020386, 'learning_rate': 3.848595550550401e-06, 'epoch': 1.85}
{'loss': 0.0877, 'grad_norm': 3.8045907020568848, 'learning_rate': 3.668535861925509e-06, 'epoch': 1.89}
{'loss': 0.0581, 'grad_norm': 1.3010648488998413, 'learning_rate': 3.4903163764736104e-06, 'epoch': 1.92}
***** Running Evaluation *****
[INFO|trainer.py:4329] 2025-07-15 12:37:36,453 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-15 12:37:36,453 >>   Batch size = 1
 75%|██████████████████████████████████████████████████▋                 | 700/939 [3:34:55<1:04:46, 16.26s/it][INFO|trainer.py:4327] 2025-07-15 13:07:55,158 >>
***** Running Evaluation *****                                                                                 
{'eval_loss': 0.08198267966508865, 'eval_accuracy': 0.959, 'eval_runtime': 127.7397, 'eval_samples_per_second': 7.828, 'eval_steps_per_second': 7.828, 'epoch': 1.92}
{'loss': 0.1032, 'grad_norm': 5.361597061157227, 'learning_rate': 3.314183409550293e-06, 'epoch': 1.95}
{'loss': 0.1026, 'grad_norm': 3.088688373565674, 'learning_rate': 3.140380392754901e-06, 'epoch': 1.98}
{'loss': 0.091, 'grad_norm': 3.4526007175445557, 'learning_rate': 2.969147537486175e-06, 'epoch': 2.01}
{'loss': 0.1225, 'grad_norm': 5.9459710121154785, 'learning_rate': 2.800721502948506e-06, 'epoch': 2.04}
{'loss': 0.0593, 'grad_norm': 2.138503313064575, 'learning_rate': 2.635335069067617e-06, 'epoch': 2.08}
{'loss': 0.0578, 'grad_norm': 0.5976075530052185, 'learning_rate': 2.4732168147677927e-06, 'epoch': 2.11}
{'loss': 0.0763, 'grad_norm': 3.650984287261963, 'learning_rate': 2.314590802055232e-06, 'epoch': 2.14}
{'loss': 0.0721, 'grad_norm': 1.3999943733215332, 'learning_rate': 2.159676266344222e-06, 'epoch': 2.17}
{'loss': 0.0611, 'grad_norm': 1.7480368614196777, 'learning_rate': 2.0086873134540626e-06, 'epoch': 2.2}
{'loss': 0.0412, 'grad_norm': 0.20524320006370544, 'learning_rate': 1.8618326236955908e-06, 'epoch': 2.24}
[INFO|trainer.py:4329] 2025-07-15 13:07:55,158 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-15 13:07:55,158 >>   Batch size = 1
 85%|███████████████████████████████████████████████████████████▋          | 800/939 [4:06:36<44:43, 19.30s/it][INFO|trainer.py:4327] 2025-07-15 13:39:35,234 >>
***** Running Evaluation *****                                                                                 
{'eval_loss': 0.083511121571064, 'eval_accuracy': 0.956, 'eval_runtime': 126.4932, 'eval_samples_per_second': 7.906, 'eval_steps_per_second': 7.906, 'epoch': 2.24}
{'loss': 0.071, 'grad_norm': 3.0181496143341064, 'learning_rate': 1.7193151634562071e-06, 'epoch': 2.27}
{'loss': 0.0672, 'grad_norm': 3.933872699737549, 'learning_rate': 1.581331904682089e-06, 'epoch': 2.3}
{'loss': 0.0653, 'grad_norm': 7.911931037902832, 'learning_rate': 1.4480735526452427e-06, 'epoch': 2.33}
{'loss': 0.0873, 'grad_norm': 0.06921537965536118, 'learning_rate': 1.319724282371664e-06, 'epoch': 2.36}
{'loss': 0.0562, 'grad_norm': 3.734361171722412, 'learning_rate': 1.1964614840949002e-06, 'epoch': 2.4}
{'loss': 0.0689, 'grad_norm': 0.9732238054275513, 'learning_rate': 1.078455518086784e-06, 'epoch': 2.43}
{'loss': 0.0873, 'grad_norm': 1.8051133155822754, 'learning_rate': 9.658694792042284e-07, 'epoch': 2.46}
{'loss': 0.0547, 'grad_norm': 5.305871486663818, 'learning_rate': 8.58858971477457e-07, 'epoch': 2.49}
{'loss': 0.0537, 'grad_norm': 2.2955963611602783, 'learning_rate': 7.575718930512516e-07, 'epoch': 2.52}
{'loss': 0.0582, 'grad_norm': 4.641275882720947, 'learning_rate': 6.621482317764105e-07, 'epoch': 2.56}
[INFO|trainer.py:4329] 2025-07-15 13:39:35,235 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-15 13:39:35,235 >>   Batch size = 1
 96%|███████████████████████████████████████████████████████████████████   | 900/939 [4:38:10<10:53, 16.76s/it][INFO|trainer.py:4327] 2025-07-15 14:11:09,344 >>
***** Running Evaluation *****                                                                                 
{'eval_loss': 0.0847933441400528, 'eval_accuracy': 0.958, 'eval_runtime': 126.6808, 'eval_samples_per_second': 7.894, 'eval_steps_per_second': 7.894, 'epoch': 2.56}
{'loss': 0.043, 'grad_norm': 2.9372339248657227, 'learning_rate': 5.727198717339511e-07, 'epoch': 2.59}
{'loss': 0.0973, 'grad_norm': 2.0016489028930664, 'learning_rate': 4.894104109594466e-07, 'epoch': 2.62}
{'loss': 0.097, 'grad_norm': 2.5692076683044434, 'learning_rate': 4.123349906194357e-07, 'epoch': 2.65}
{'loss': 0.0456, 'grad_norm': 1.6541318893432617, 'learning_rate': 3.416001358759635e-07, 'epoch': 2.68}
{'loss': 0.0215, 'grad_norm': 0.971393883228302, 'learning_rate': 2.7730360865923954e-07, 'epoch': 2.72}
{'loss': 0.0645, 'grad_norm': 0.5602942705154419, 'learning_rate': 2.1953427255185122e-07, 'epoch': 2.75}
{'loss': 0.0579, 'grad_norm': 1.3743181228637695, 'learning_rate': 1.6837196997130434e-07, 'epoch': 2.78}
{'loss': 0.0829, 'grad_norm': 0.921851634979248, 'learning_rate': 1.2388741182062348e-07, 'epoch': 2.81}
{'loss': 0.0418, 'grad_norm': 1.1836774349212646, 'learning_rate': 8.614207975952083e-08, 'epoch': 2.84}
{'loss': 0.0498, 'grad_norm': 3.0185816287994385, 'learning_rate': 5.518814123121885e-08, 'epoch': 2.88}
[INFO|trainer.py:4329] 2025-07-15 14:11:09,345 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-15 14:11:09,345 >>   Batch size = 1
100%|██████████████████████████████████████████████████████████████████████| 939/939 [4:50:56<00:00, 15.27s/it][INFO|trainer.py:3993] 2025-07-15 14:23:55,739 >> Saving model checkpoint to saves/golden/user1/toy_reward/checkpoint-939
[INFO|trainer.py:4007] 2025-07-15 14:23:55,743 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
{'eval_loss': 0.08469186723232269, 'eval_accuracy': 0.959, 'eval_runtime': 126.8757, 'eval_samples_per_second': 7.882, 'eval_steps_per_second': 7.882, 'epoch': 2.88}
{'loss': 0.0664, 'grad_norm': 2.0196993350982666, 'learning_rate': 3.10683773623488e-08, 'epoch': 2.91}
{'loss': 0.053, 'grad_norm': 4.018370628356934, 'learning_rate': 1.3816123835588835e-08, 'epoch': 2.94}
{'loss': 0.0699, 'grad_norm': 0.6543918251991272, 'learning_rate': 3.4552248167507576e-09, 'epoch': 2.97}
[INFO|tokenization_utils_base.py:2356] 2025-07-15 14:23:55,750 >> chat template saved in saves/golden/user1/toy_reward/checkpoint-939/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-15 14:23:55,753 >> tokenizer config file saved in saves/golden/user1/toy_reward/checkpoint-939/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-15 14:23:55,753 >> Special tokens file saved in saves/golden/user1/toy_reward/checkpoint-939/special_tokens_map.json
[INFO|configuration_utils.py:698] 2025-07-15 14:23:56,984 >> loading configuration file config.json from cache at /mmfs1/gscratch/ark/devinl6/hf_cache/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:770] 2025-07-15 14:23:56,986 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "vocab_size": 128256
}
[INFO|2025-07-15 14:23:57] llamafactory.train.callbacks:143 >> Value head model saved at: saves/golden/user1/toy_reward/checkpoint-939

[INFO|trainer.py:2676] 2025-07-15 14:23:57,117 >>

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|██████████████████████████████████████████████████████████████████████| 939/939 [4:50:57<00:00, 18.59s/it]
{'train_runtime': 17459.5644, 'train_samples_per_second': 1.718, 'train_steps_per_second': 0.054, 'train_loss': 0.15036464851504316, 'epoch': 3.0}
[INFO|trainer.py:3993] 2025-07-15 14:23:57,120 >> Saving model checkpoint to saves/golden/user1/toy_reward
[INFO|trainer.py:4007] 2025-07-15 14:23:57,124 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|tokenization_utils_base.py:2356] 2025-07-15 14:23:57,128 >> chat template saved in saves/golden/user1/toy_reward/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-15 14:23:57,131 >> tokenizer config file saved in saves/golden/user1/toy_reward/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-15 14:23:57,131 >> Special tokens file saved in saves/golden/user1/toy_reward/special_tokens_map.json
[INFO|configuration_utils.py:698] 2025-07-15 14:23:57,743 >> loading configuration file config.json from cache at /mmfs1/gscratch/ark/devinl6/hf_cache/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:770] 2025-07-15 14:23:57,744 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "vocab_size": 128256
}
[INFO|2025-07-15 14:23:57] llamafactory.train.callbacks:143 >> Value head model saved at: saves/golden/user1/toy_reward
***** train metrics *****
  epoch                    =        3.0
  total_flos               =        0GF
  train_loss               =     0.1504
  train_runtime            = 4:50:59.56
  train_samples_per_second =      1.718
  train_steps_per_second   =      0.054
Figure saved at: saves/golden/user1/toy_reward/training_loss.png
Figure saved at: saves/golden/user1/toy_reward/training_eval_loss.png
Figure saved at: saves/golden/user1/toy_reward/training_eval_accuracy.png

[INFO|trainer.py:4327] 2025-07-15 14:23:58,679 >>
***** Running Evaluation *****
[INFO|trainer.py:4329] 2025-07-15 14:23:58,679 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-15 14:23:58,679 >>   Batch size = 1
100%|██████████████████████████████████████████████████████████████████████| 1000/1000 [02:06<00:00,  7.94it/s]
***** eval metrics *****
  epoch                   =        3.0
  eval_accuracy           =      0.959
  eval_loss               =     0.0842
  eval_runtime            = 0:02:06.37
  eval_samples_per_second =      7.913
  eval_steps_per_second   =      7.913
[INFO|modelcard.py:450] 2025-07-15 14:26:05,058 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.959}]}
