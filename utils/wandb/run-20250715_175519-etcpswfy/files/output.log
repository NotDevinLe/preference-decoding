[INFO|integration_utils.py:880] 2025-07-15 17:55:20,175 >> Could not log the number of model parameters in Weights & Biases due to an AttributeError.
 11%|█         | 100/939 [28:02<3:42:55, 15.94s/it][INFO|trainer.py:4327] 2025-07-15 18:23:22,249 >>
{'loss': 0.7295, 'grad_norm': 9.023003578186035, 'learning_rate': 9.574468085106384e-07, 'epoch': 0.03}
{'loss': 0.7483, 'grad_norm': 9.510113716125488, 'learning_rate': 2.021276595744681e-06, 'epoch': 0.06}
{'loss': 0.7728, 'grad_norm': 10.09823226928711, 'learning_rate': 3.0851063829787237e-06, 'epoch': 0.1}
{'loss': 0.6969, 'grad_norm': 10.034303665161133, 'learning_rate': 4.148936170212766e-06, 'epoch': 0.13}
{'loss': 0.6744, 'grad_norm': 7.465901851654053, 'learning_rate': 5.212765957446809e-06, 'epoch': 0.16}
{'loss': 0.5349, 'grad_norm': 6.784566879272461, 'learning_rate': 6.276595744680851e-06, 'epoch': 0.19}
{'loss': 0.4477, 'grad_norm': 5.9366455078125, 'learning_rate': 7.340425531914894e-06, 'epoch': 0.22}
{'loss': 0.3149, 'grad_norm': 3.954188585281372, 'learning_rate': 8.404255319148937e-06, 'epoch': 0.26}
{'loss': 0.1945, 'grad_norm': 3.6148874759674072, 'learning_rate': 9.46808510638298e-06, 'epoch': 0.29}
{'loss': 0.1859, 'grad_norm': 4.7873759269714355, 'learning_rate': 9.999136119166803e-06, 'epoch': 0.32}
***** Running Evaluation *****
[INFO|trainer.py:4329] 2025-07-15 18:23:22,249 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-15 18:23:22,249 >>   Batch size = 1
 21%|██▏       | 200/939 [58:07<3:22:40, 16.45s/it][INFO|trainer.py:4327] 2025-07-15 18:53:27,265 >>
***** Running Evaluation *****                     
{'eval_loss': 0.21316775679588318, 'eval_accuracy': 0.906, 'eval_runtime': 119.0812, 'eval_samples_per_second': 8.398, 'eval_steps_per_second': 8.398, 'epoch': 0.32}
{'loss': 0.2073, 'grad_norm': 6.6049089431762695, 'learning_rate': 9.9922268634943e-06, 'epoch': 0.35}
{'loss': 0.1633, 'grad_norm': 6.097295761108398, 'learning_rate': 9.978417901361958e-06, 'epoch': 0.38}
{'loss': 0.153, 'grad_norm': 3.723433017730713, 'learning_rate': 9.95772831799724e-06, 'epoch': 0.42}
{'loss': 0.1734, 'grad_norm': 3.3932647705078125, 'learning_rate': 9.930186708264902e-06, 'epoch': 0.45}
{'loss': 0.1563, 'grad_norm': 2.515895128250122, 'learning_rate': 9.895831137146319e-06, 'epoch': 0.48}
{'loss': 0.1859, 'grad_norm': 5.550467014312744, 'learning_rate': 9.854709087130261e-06, 'epoch': 0.51}
{'loss': 0.1527, 'grad_norm': 3.7506184577941895, 'learning_rate': 9.80687739258782e-06, 'epoch': 0.54}
{'loss': 0.1357, 'grad_norm': 4.594451904296875, 'learning_rate': 9.7524021612222e-06, 'epoch': 0.58}
{'loss': 0.1248, 'grad_norm': 2.3253211975097656, 'learning_rate': 9.691358682701927e-06, 'epoch': 0.61}
{'loss': 0.1372, 'grad_norm': 5.611339569091797, 'learning_rate': 9.623831324603755e-06, 'epoch': 0.64}
[INFO|trainer.py:4329] 2025-07-15 18:53:27,266 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-15 18:53:27,266 >>   Batch size = 1
 32%|███▏      | 300/939 [1:29:27<3:01:54, 17.08s/it][INFO|trainer.py:4327] 2025-07-15 19:24:47,919 >>
***** Running Evaluation *****                     
{'eval_loss': 0.1285107433795929, 'eval_accuracy': 0.936, 'eval_runtime': 119.0741, 'eval_samples_per_second': 8.398, 'eval_steps_per_second': 8.398, 'epoch': 0.64}
{'loss': 0.0929, 'grad_norm': 2.191986322402954, 'learning_rate': 9.549913415809084e-06, 'epoch': 0.67}
{'loss': 0.109, 'grad_norm': 3.60086727142334, 'learning_rate': 9.469707117515068e-06, 'epoch': 0.7}
{'loss': 0.1589, 'grad_norm': 4.417534828186035, 'learning_rate': 9.383323282038632e-06, 'epoch': 0.74}
{'loss': 0.1018, 'grad_norm': 1.5965831279754639, 'learning_rate': 9.29088129960862e-06, 'epoch': 0.77}
{'loss': 0.0729, 'grad_norm': 4.643388271331787, 'learning_rate': 9.192508933357753e-06, 'epoch': 0.8}
{'loss': 0.1044, 'grad_norm': 2.7176785469055176, 'learning_rate': 9.088342142742493e-06, 'epoch': 0.83}
{'loss': 0.0808, 'grad_norm': 3.295816659927368, 'learning_rate': 8.978524895634842e-06, 'epoch': 0.86}
{'loss': 0.0958, 'grad_norm': 2.7369489669799805, 'learning_rate': 8.86320896934581e-06, 'epoch': 0.9}
{'loss': 0.0652, 'grad_norm': 3.2133986949920654, 'learning_rate': 8.742553740855507e-06, 'epoch': 0.93}
{'loss': 0.0813, 'grad_norm': 1.526286244392395, 'learning_rate': 8.616725966539831e-06, 'epoch': 0.96}
[INFO|trainer.py:4329] 2025-07-15 19:24:47,919 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-15 19:24:47,919 >>   Batch size = 1
 43%|████▎     | 400/939 [1:59:59<2:22:09, 15.82s/it][INFO|trainer.py:4327] 2025-07-15 19:55:19,428 >>
***** Running Evaluation *****                     
{'eval_loss': 0.11558044701814651, 'eval_accuracy': 0.944, 'eval_runtime': 121.1562, 'eval_samples_per_second': 8.254, 'eval_steps_per_second': 8.254, 'epoch': 0.96}
{'loss': 0.0929, 'grad_norm': 2.6193158626556396, 'learning_rate': 8.485899551698166e-06, 'epoch': 0.99}
{'loss': 0.0772, 'grad_norm': 2.1392037868499756, 'learning_rate': 8.350255310200611e-06, 'epoch': 1.02}
{'loss': 0.0976, 'grad_norm': 2.4596188068389893, 'learning_rate': 8.209980714586955e-06, 'epoch': 1.05}
{'loss': 0.0678, 'grad_norm': 2.1810355186462402, 'learning_rate': 8.065269636962765e-06, 'epoch': 1.09}
{'loss': 0.0936, 'grad_norm': 0.8133664727210999, 'learning_rate': 7.916322081050708e-06, 'epoch': 1.12}
{'loss': 0.059, 'grad_norm': 4.691857814788818, 'learning_rate': 7.76334390576742e-06, 'epoch': 1.15}
{'loss': 0.0642, 'grad_norm': 4.594664096832275, 'learning_rate': 7.60654654070796e-06, 'epoch': 1.18}
{'loss': 0.0851, 'grad_norm': 1.852408766746521, 'learning_rate': 7.446146693931111e-06, 'epoch': 1.21}
{'loss': 0.1263, 'grad_norm': 11.82029914855957, 'learning_rate': 7.282366052449351e-06, 'epoch': 1.25}
{'loss': 0.1008, 'grad_norm': 2.9956634044647217, 'learning_rate': 7.115430975837457e-06, 'epoch': 1.28}
[INFO|trainer.py:4329] 2025-07-15 19:55:19,429 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-15 19:55:19,429 >>   Batch size = 1
 53%|█████▎    | 500/939 [2:30:03<1:58:03, 16.14s/it][INFO|trainer.py:4327] 2025-07-15 20:25:23,947 >>
***** Running Evaluation *****                     
{'eval_loss': 0.11185423284769058, 'eval_accuracy': 0.941, 'eval_runtime': 121.3174, 'eval_samples_per_second': 8.243, 'eval_steps_per_second': 8.243, 'epoch': 1.28}
{'loss': 0.0931, 'grad_norm': 2.849679946899414, 'learning_rate': 6.945572183383229e-06, 'epoch': 1.31}
{'loss': 0.0773, 'grad_norm': 2.707364797592163, 'learning_rate': 6.773024435212678e-06, 'epoch': 1.34}
{'loss': 0.0913, 'grad_norm': 3.623598098754883, 'learning_rate': 6.598026207830428e-06, 'epoch': 1.37}
{'loss': 0.0897, 'grad_norm': 3.0959279537200928, 'learning_rate': 6.4208193645237314e-06, 'epoch': 1.41}
{'loss': 0.1239, 'grad_norm': 5.789974212646484, 'learning_rate': 6.241648821085666e-06, 'epoch': 1.44}
{'loss': 0.0675, 'grad_norm': 5.371641635894775, 'learning_rate': 6.060762207319479e-06, 'epoch': 1.47}
{'loss': 0.0793, 'grad_norm': 1.3131119012832642, 'learning_rate': 5.878409524791931e-06, 'epoch': 1.5}
{'loss': 0.1054, 'grad_norm': 1.5388494729995728, 'learning_rate': 5.694842801308651e-06, 'epoch': 1.53}
{'loss': 0.1219, 'grad_norm': 3.221359968185425, 'learning_rate': 5.510315742589042e-06, 'epoch': 1.57}
{'loss': 0.0876, 'grad_norm': 1.2063226699829102, 'learning_rate': 5.325083381622165e-06, 'epoch': 1.6}
[INFO|trainer.py:4329] 2025-07-15 20:25:23,947 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-15 20:25:23,947 >>   Batch size = 1
 53%|█████▎    | 500/939 [2:32:02<1:58:03, 16.14s/i[INFO|trainer.py:3993] 2025-07-15 20:27:22,318 >> Saving model checkpoint to saves/golden/user3/toy_reward/checkpoint-500
[INFO|trainer.py:4007] 2025-07-15 20:27:22,322 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
{'eval_loss': 0.11641337722539902, 'eval_accuracy': 0.945, 'eval_runtime': 118.369, 'eval_samples_per_second': 8.448, 'eval_steps_per_second': 8.448, 'epoch': 1.6}
[INFO|tokenization_utils_base.py:2356] 2025-07-15 20:27:22,388 >> chat template saved in saves/golden/user3/toy_reward/checkpoint-500/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-15 20:27:22,391 >> tokenizer config file saved in saves/golden/user3/toy_reward/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-15 20:27:22,392 >> Special tokens file saved in saves/golden/user3/toy_reward/checkpoint-500/special_tokens_map.json
[INFO|configuration_utils.py:698] 2025-07-15 20:27:23,598 >> loading configuration file config.json from cache at /mmfs1/gscratch/ark/devinl6/hf_cache/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:770] 2025-07-15 20:27:23,599 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "vocab_size": 128256
}
[INFO|2025-07-15 20:27:23] llamafactory.train.callbacks:143 >> Value head model saved at: saves/golden/user3/toy_reward/checkpoint-500

 64%|██████▍   | 600/939 [3:00:42<1:34:27, 16.72s/it][INFO|trainer.py:4327] 2025-07-15 20:56:02,411 >>
{'loss': 0.0968, 'grad_norm': 6.097723484039307, 'learning_rate': 5.139401726188208e-06, 'epoch': 1.63}
{'loss': 0.074, 'grad_norm': 5.472105026245117, 'learning_rate': 4.953527405032723e-06, 'epoch': 1.66}
{'loss': 0.0477, 'grad_norm': 2.378312349319458, 'learning_rate': 4.767717313182611e-06, 'epoch': 1.69}
{'loss': 0.0775, 'grad_norm': 5.494696617126465, 'learning_rate': 4.582228256894093e-06, 'epoch': 1.73}
{'loss': 0.0856, 'grad_norm': 1.0903429985046387, 'learning_rate': 4.397316598723385e-06, 'epoch': 1.76}
{'loss': 0.0775, 'grad_norm': 0.7409801483154297, 'learning_rate': 4.2132379032105695e-06, 'epoch': 1.79}
{'loss': 0.0525, 'grad_norm': 1.6841636896133423, 'learning_rate': 4.030246583666437e-06, 'epoch': 1.82}
{'loss': 0.0789, 'grad_norm': 1.449795126914978, 'learning_rate': 3.848595550550401e-06, 'epoch': 1.85}
{'loss': 0.0382, 'grad_norm': 0.4830702245235443, 'learning_rate': 3.668535861925509e-06, 'epoch': 1.89}
{'loss': 0.0873, 'grad_norm': 1.9841336011886597, 'learning_rate': 3.4903163764736104e-06, 'epoch': 1.92}
***** Running Evaluation *****
[INFO|trainer.py:4329] 2025-07-15 20:56:02,411 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-15 20:56:02,411 >>   Batch size = 1
 75%|███████▍  | 700/939 [3:31:22<56:34, 14.20s/it][INFO|trainer.py:4327] 2025-07-15 21:26:42,969 >>
***** Running Evaluation *****                     
{'eval_loss': 0.133774995803833, 'eval_accuracy': 0.945, 'eval_runtime': 118.4123, 'eval_samples_per_second': 8.445, 'eval_steps_per_second': 8.445, 'epoch': 1.92}
{'loss': 0.0636, 'grad_norm': 6.002810478210449, 'learning_rate': 3.314183409550293e-06, 'epoch': 1.95}
{'loss': 0.127, 'grad_norm': 6.321369171142578, 'learning_rate': 3.140380392754901e-06, 'epoch': 1.98}
{'loss': 0.1035, 'grad_norm': 2.244619846343994, 'learning_rate': 2.969147537486175e-06, 'epoch': 2.01}
{'loss': 0.0654, 'grad_norm': 1.3629043102264404, 'learning_rate': 2.800721502948506e-06, 'epoch': 2.04}
{'loss': 0.0586, 'grad_norm': 6.758049488067627, 'learning_rate': 2.635335069067617e-06, 'epoch': 2.08}
{'loss': 0.0389, 'grad_norm': 0.9180459976196289, 'learning_rate': 2.4732168147677927e-06, 'epoch': 2.11}
{'loss': 0.0384, 'grad_norm': 2.2808821201324463, 'learning_rate': 2.314590802055232e-06, 'epoch': 2.14}
{'loss': 0.1027, 'grad_norm': 7.848576545715332, 'learning_rate': 2.159676266344222e-06, 'epoch': 2.17}
{'loss': 0.0581, 'grad_norm': 7.606435775756836, 'learning_rate': 2.0086873134540626e-06, 'epoch': 2.2}
{'loss': 0.0838, 'grad_norm': 6.010382175445557, 'learning_rate': 1.8618326236955908e-06, 'epoch': 2.24}
[INFO|trainer.py:4329] 2025-07-15 21:26:42,969 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-15 21:26:42,969 >>   Batch size = 1
 85%|████████▌ | 800/939 [4:00:59<39:45, 17.16s/it][INFO|trainer.py:4327] 2025-07-15 21:56:19,609 >>
***** Running Evaluation *****                     
{'eval_loss': 0.12935464084148407, 'eval_accuracy': 0.946, 'eval_runtime': 118.3328, 'eval_samples_per_second': 8.451, 'eval_steps_per_second': 8.451, 'epoch': 2.24}
{'loss': 0.0607, 'grad_norm': 1.2666254043579102, 'learning_rate': 1.7193151634562071e-06, 'epoch': 2.27}
{'loss': 0.0876, 'grad_norm': 5.5448479652404785, 'learning_rate': 1.581331904682089e-06, 'epoch': 2.3}
{'loss': 0.0446, 'grad_norm': 1.0485126972198486, 'learning_rate': 1.4480735526452427e-06, 'epoch': 2.33}
{'loss': 0.0565, 'grad_norm': 2.104135513305664, 'learning_rate': 1.319724282371664e-06, 'epoch': 2.36}
{'loss': 0.0646, 'grad_norm': 3.5792253017425537, 'learning_rate': 1.1964614840949002e-06, 'epoch': 2.4}
{'loss': 0.0517, 'grad_norm': 2.915127992630005, 'learning_rate': 1.078455518086784e-06, 'epoch': 2.43}
{'loss': 0.03, 'grad_norm': 0.8554193377494812, 'learning_rate': 9.658694792042284e-07, 'epoch': 2.46}
{'loss': 0.0557, 'grad_norm': 3.2385964393615723, 'learning_rate': 8.58858971477457e-07, 'epoch': 2.49}
{'loss': 0.0431, 'grad_norm': 4.3368377685546875, 'learning_rate': 7.575718930512516e-07, 'epoch': 2.52}
{'loss': 0.0551, 'grad_norm': 1.527938723564148, 'learning_rate': 6.621482317764105e-07, 'epoch': 2.56}
[INFO|trainer.py:4329] 2025-07-15 21:56:19,609 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-15 21:56:19,609 >>   Batch size = 1
 96%|█████████▌| 900/939 [4:31:19<09:34, 14.73s/it][INFO|trainer.py:4327] 2025-07-15 22:26:40,050 >>
***** Running Evaluation *****                     
{'eval_loss': 0.1322919726371765, 'eval_accuracy': 0.948, 'eval_runtime': 119.0935, 'eval_samples_per_second': 8.397, 'eval_steps_per_second': 8.397, 'epoch': 2.56}
{'loss': 0.0648, 'grad_norm': 5.961442947387695, 'learning_rate': 5.727198717339511e-07, 'epoch': 2.59}
{'loss': 0.0578, 'grad_norm': 3.0767838954925537, 'learning_rate': 4.894104109594466e-07, 'epoch': 2.62}
{'loss': 0.0641, 'grad_norm': 1.732094168663025, 'learning_rate': 4.123349906194357e-07, 'epoch': 2.65}
{'loss': 0.0654, 'grad_norm': 6.346716403961182, 'learning_rate': 3.416001358759635e-07, 'epoch': 2.68}
{'loss': 0.0516, 'grad_norm': 5.023981094360352, 'learning_rate': 2.7730360865923954e-07, 'epoch': 2.72}
{'loss': 0.0481, 'grad_norm': 1.4845465421676636, 'learning_rate': 2.1953427255185122e-07, 'epoch': 2.75}
{'loss': 0.0592, 'grad_norm': 3.7132601737976074, 'learning_rate': 1.6837196997130434e-07, 'epoch': 2.78}
{'loss': 0.074, 'grad_norm': 0.2538395822048187, 'learning_rate': 1.2388741182062348e-07, 'epoch': 2.81}
{'loss': 0.0999, 'grad_norm': 3.9981391429901123, 'learning_rate': 8.614207975952083e-08, 'epoch': 2.84}
{'loss': 0.0585, 'grad_norm': 3.0785670280456543, 'learning_rate': 5.518814123121885e-08, 'epoch': 2.88}
[INFO|trainer.py:4329] 2025-07-15 22:26:40,050 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-15 22:26:40,050 >>   Batch size = 1
100%|██████████| 939/939 [4:44:33<00:00, 15.12s/it][INFO|trainer.py:3993] 2025-07-15 22:39:54,177 >> Saving model checkpoint to saves/golden/user3/toy_reward/checkpoint-939
[INFO|trainer.py:4007] 2025-07-15 22:39:54,181 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
{'eval_loss': 0.13160479068756104, 'eval_accuracy': 0.945, 'eval_runtime': 121.1565, 'eval_samples_per_second': 8.254, 'eval_steps_per_second': 8.254, 'epoch': 2.88}
{'loss': 0.0509, 'grad_norm': 1.233130931854248, 'learning_rate': 3.10683773623488e-08, 'epoch': 2.91}
{'loss': 0.0495, 'grad_norm': 7.091444492340088, 'learning_rate': 1.3816123835588835e-08, 'epoch': 2.94}
{'loss': 0.0938, 'grad_norm': 0.1193118691444397, 'learning_rate': 3.4552248167507576e-09, 'epoch': 2.97}
[INFO|tokenization_utils_base.py:2356] 2025-07-15 22:39:54,379 >> chat template saved in saves/golden/user3/toy_reward/checkpoint-939/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-15 22:39:54,383 >> tokenizer config file saved in saves/golden/user3/toy_reward/checkpoint-939/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-15 22:39:54,383 >> Special tokens file saved in saves/golden/user3/toy_reward/checkpoint-939/special_tokens_map.json
[INFO|configuration_utils.py:698] 2025-07-15 22:39:55,872 >> loading configuration file config.json from cache at /mmfs1/gscratch/ark/devinl6/hf_cache/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:770] 2025-07-15 22:39:55,873 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "vocab_size": 128256
}
[INFO|2025-07-15 22:39:55] llamafactory.train.callbacks:143 >> Value head model saved at: saves/golden/user3/toy_reward/checkpoint-939

[INFO|trainer.py:2676] 2025-07-15 22:39:55,990 >>

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|██████████| 939/939 [4:44:35<00:00, 18.19s/it]
{'train_runtime': 17077.1797, 'train_samples_per_second': 1.757, 'train_steps_per_second': 0.055, 'train_loss': 0.133654939671294, 'epoch': 3.0}
[INFO|trainer.py:3993] 2025-07-15 22:39:55,993 >> Saving model checkpoint to saves/golden/user3/toy_reward
[INFO|trainer.py:4007] 2025-07-15 22:39:55,997 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|tokenization_utils_base.py:2356] 2025-07-15 22:39:56,000 >> chat template saved in saves/golden/user3/toy_reward/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-15 22:39:56,004 >> tokenizer config file saved in saves/golden/user3/toy_reward/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-15 22:39:56,005 >> Special tokens file saved in saves/golden/user3/toy_reward/special_tokens_map.json
[INFO|configuration_utils.py:698] 2025-07-15 22:39:56,540 >> loading configuration file config.json from cache at /mmfs1/gscratch/ark/devinl6/hf_cache/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:770] 2025-07-15 22:39:56,541 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "vocab_size": 128256
}
[INFO|2025-07-15 22:39:56] llamafactory.train.callbacks:143 >> Value head model saved at: saves/golden/user3/toy_reward
***** train metrics *****
  epoch                    =        3.0
  total_flos               =        0GF
  train_loss               =     0.1337
  train_runtime            = 4:44:37.17
  train_samples_per_second =      1.757
  train_steps_per_second   =      0.055
Figure saved at: saves/golden/user3/toy_reward/training_loss.png
Figure saved at: saves/golden/user3/toy_reward/training_eval_loss.png
Figure saved at: saves/golden/user3/toy_reward/training_eval_accuracy.png

[INFO|trainer.py:4327] 2025-07-15 22:40:04,005 >>
***** Running Evaluation *****
[INFO|trainer.py:4329] 2025-07-15 22:40:04,005 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-15 22:40:04,005 >>   Batch size = 1
100%|██████████| 1000/1000 [01:58<00:00,  8.43it/s]
***** eval metrics *****
  epoch                   =        3.0
  eval_accuracy           =      0.947
  eval_loss               =     0.1321
  eval_runtime            = 0:01:59.11
  eval_samples_per_second =      8.395
  eval_steps_per_second   =      8.395
[INFO|modelcard.py:450] 2025-07-15 22:42:03,118 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.947}]}
