[INFO|integration_utils.py:880] 2025-07-15 17:56:42,151 >> Could not log the number of model parameters in Weights & Biases due to an AttributeError.
 11%|█         | 100/939 [29:16<4:05:47, 17.58s/it][INFO|trainer.py:4327] 2025-07-15 18:25:58,421 >>
{'loss': 0.7612, 'grad_norm': 10.05296802520752, 'learning_rate': 9.574468085106384e-07, 'epoch': 0.03}
{'loss': 0.7563, 'grad_norm': 9.349720001220703, 'learning_rate': 2.021276595744681e-06, 'epoch': 0.06}
{'loss': 0.7782, 'grad_norm': 12.342247009277344, 'learning_rate': 3.0851063829787237e-06, 'epoch': 0.1}
{'loss': 0.7575, 'grad_norm': 10.64119815826416, 'learning_rate': 4.148936170212766e-06, 'epoch': 0.13}
{'loss': 0.6644, 'grad_norm': 10.688407897949219, 'learning_rate': 5.212765957446809e-06, 'epoch': 0.16}
{'loss': 0.5159, 'grad_norm': 6.033334255218506, 'learning_rate': 6.276595744680851e-06, 'epoch': 0.19}
{'loss': 0.3617, 'grad_norm': 5.10055685043335, 'learning_rate': 7.340425531914894e-06, 'epoch': 0.22}
{'loss': 0.2512, 'grad_norm': 4.492670059204102, 'learning_rate': 8.404255319148937e-06, 'epoch': 0.26}
{'loss': 0.2257, 'grad_norm': 5.553348064422607, 'learning_rate': 9.46808510638298e-06, 'epoch': 0.29}
{'loss': 0.1781, 'grad_norm': 3.651553153991699, 'learning_rate': 9.999136119166803e-06, 'epoch': 0.32}
***** Running Evaluation *****
[INFO|trainer.py:4329] 2025-07-15 18:25:58,421 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-15 18:25:58,421 >>   Batch size = 1
 21%|██▏       | 200/939 [1:00:17<3:04:35, 14.99s/it][INFO|trainer.py:4327] 2025-07-15 18:57:00,124 >>
***** Running Evaluation *****                     
{'eval_loss': 0.19313205778598785, 'eval_accuracy': 0.894, 'eval_runtime': 126.3379, 'eval_samples_per_second': 7.915, 'eval_steps_per_second': 7.915, 'epoch': 0.32}
{'loss': 0.1215, 'grad_norm': 3.371427536010742, 'learning_rate': 9.9922268634943e-06, 'epoch': 0.35}
{'loss': 0.1835, 'grad_norm': 5.622176170349121, 'learning_rate': 9.978417901361958e-06, 'epoch': 0.38}
{'loss': 0.1894, 'grad_norm': 4.778481483459473, 'learning_rate': 9.95772831799724e-06, 'epoch': 0.42}
{'loss': 0.155, 'grad_norm': 4.242613315582275, 'learning_rate': 9.930186708264902e-06, 'epoch': 0.45}
{'loss': 0.2201, 'grad_norm': 5.088045597076416, 'learning_rate': 9.895831137146319e-06, 'epoch': 0.48}
{'loss': 0.1428, 'grad_norm': 3.6573328971862793, 'learning_rate': 9.854709087130261e-06, 'epoch': 0.51}
{'loss': 0.1498, 'grad_norm': 2.944143772125244, 'learning_rate': 9.80687739258782e-06, 'epoch': 0.54}
{'loss': 0.1863, 'grad_norm': 3.3911492824554443, 'learning_rate': 9.7524021612222e-06, 'epoch': 0.58}
{'loss': 0.1153, 'grad_norm': 1.9259412288665771, 'learning_rate': 9.691358682701927e-06, 'epoch': 0.61}
{'loss': 0.1986, 'grad_norm': 4.024397850036621, 'learning_rate': 9.623831324603755e-06, 'epoch': 0.64}
[INFO|trainer.py:4329] 2025-07-15 18:57:00,124 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-15 18:57:00,124 >>   Batch size = 1
 32%|███▏      | 300/939 [1:31:48<2:57:45, 16.69s/it][INFO|trainer.py:4327] 2025-07-15 19:28:30,986 >>
***** Running Evaluation *****                     
{'eval_loss': 0.14937038719654083, 'eval_accuracy': 0.913, 'eval_runtime': 126.0252, 'eval_samples_per_second': 7.935, 'eval_steps_per_second': 7.935, 'epoch': 0.64}
{'loss': 0.1602, 'grad_norm': 3.5907633304595947, 'learning_rate': 9.549913415809084e-06, 'epoch': 0.67}
{'loss': 0.149, 'grad_norm': 4.554319381713867, 'learning_rate': 9.469707117515068e-06, 'epoch': 0.7}
{'loss': 0.1498, 'grad_norm': 2.8813247680664062, 'learning_rate': 9.383323282038632e-06, 'epoch': 0.74}
{'loss': 0.1557, 'grad_norm': 3.5019147396087646, 'learning_rate': 9.29088129960862e-06, 'epoch': 0.77}
{'loss': 0.1204, 'grad_norm': 2.785792350769043, 'learning_rate': 9.192508933357753e-06, 'epoch': 0.8}
{'loss': 0.174, 'grad_norm': 3.3988945484161377, 'learning_rate': 9.088342142742493e-06, 'epoch': 0.83}
{'loss': 0.1218, 'grad_norm': 2.0714454650878906, 'learning_rate': 8.978524895634842e-06, 'epoch': 0.86}
{'loss': 0.124, 'grad_norm': 4.565857410430908, 'learning_rate': 8.86320896934581e-06, 'epoch': 0.9}
{'loss': 0.1861, 'grad_norm': 5.222728252410889, 'learning_rate': 8.742553740855507e-06, 'epoch': 0.93}
{'loss': 0.1162, 'grad_norm': 3.7046761512756348, 'learning_rate': 8.616725966539831e-06, 'epoch': 0.96}
[INFO|trainer.py:4329] 2025-07-15 19:28:30,987 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-15 19:28:30,987 >>   Batch size = 1
 43%|████▎     | 400/939 [2:03:01<2:36:02, 17.37s/it][INFO|trainer.py:4327] 2025-07-15 19:59:43,479 >>
***** Running Evaluation *****                     
{'eval_loss': 0.13609828054904938, 'eval_accuracy': 0.917, 'eval_runtime': 124.9119, 'eval_samples_per_second': 8.006, 'eval_steps_per_second': 8.006, 'epoch': 0.96}
{'loss': 0.1566, 'grad_norm': 3.6830124855041504, 'learning_rate': 8.485899551698166e-06, 'epoch': 0.99}
{'loss': 0.1326, 'grad_norm': 1.534401774406433, 'learning_rate': 8.350255310200611e-06, 'epoch': 1.02}
{'loss': 0.1209, 'grad_norm': 1.4181859493255615, 'learning_rate': 8.209980714586955e-06, 'epoch': 1.05}
{'loss': 0.103, 'grad_norm': 2.4145991802215576, 'learning_rate': 8.065269636962765e-06, 'epoch': 1.09}
{'loss': 0.1731, 'grad_norm': 3.506688117980957, 'learning_rate': 7.916322081050708e-06, 'epoch': 1.12}
{'loss': 0.1605, 'grad_norm': 2.156217336654663, 'learning_rate': 7.76334390576742e-06, 'epoch': 1.15}
{'loss': 0.1293, 'grad_norm': 3.7571792602539062, 'learning_rate': 7.60654654070796e-06, 'epoch': 1.18}
{'loss': 0.1448, 'grad_norm': 2.3387434482574463, 'learning_rate': 7.446146693931111e-06, 'epoch': 1.21}
{'loss': 0.1212, 'grad_norm': 3.530233144760132, 'learning_rate': 7.282366052449351e-06, 'epoch': 1.25}
{'loss': 0.1366, 'grad_norm': 2.2633004188537598, 'learning_rate': 7.115430975837457e-06, 'epoch': 1.28}
[INFO|trainer.py:4329] 2025-07-15 19:59:43,480 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-15 19:59:43,480 >>   Batch size = 1
 53%|█████▎    | 500/939 [2:34:42<2:32:24, 20.83s/it][INFO|trainer.py:4327] 2025-07-15 20:31:24,407 >>
***** Running Evaluation *****                     
{'eval_loss': 0.13701175153255463, 'eval_accuracy': 0.915, 'eval_runtime': 127.9325, 'eval_samples_per_second': 7.817, 'eval_steps_per_second': 7.817, 'epoch': 1.28}
{'loss': 0.1458, 'grad_norm': 2.780045509338379, 'learning_rate': 6.945572183383229e-06, 'epoch': 1.31}
{'loss': 0.1444, 'grad_norm': 2.806837558746338, 'learning_rate': 6.773024435212678e-06, 'epoch': 1.34}
{'loss': 0.1202, 'grad_norm': 4.077398300170898, 'learning_rate': 6.598026207830428e-06, 'epoch': 1.37}
{'loss': 0.1447, 'grad_norm': 2.612382173538208, 'learning_rate': 6.4208193645237314e-06, 'epoch': 1.41}
{'loss': 0.1139, 'grad_norm': 3.6858432292938232, 'learning_rate': 6.241648821085666e-06, 'epoch': 1.44}
{'loss': 0.1243, 'grad_norm': 3.95599627494812, 'learning_rate': 6.060762207319479e-06, 'epoch': 1.47}
{'loss': 0.0986, 'grad_norm': 1.2085150480270386, 'learning_rate': 5.878409524791931e-06, 'epoch': 1.5}
{'loss': 0.124, 'grad_norm': 3.1243484020233154, 'learning_rate': 5.694842801308651e-06, 'epoch': 1.53}
{'loss': 0.0969, 'grad_norm': 4.307900905609131, 'learning_rate': 5.510315742589042e-06, 'epoch': 1.57}
{'loss': 0.137, 'grad_norm': 4.2495832443237305, 'learning_rate': 5.325083381622165e-06, 'epoch': 1.6}
[INFO|trainer.py:4329] 2025-07-15 20:31:24,407 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-15 20:31:24,407 >>   Batch size = 1
 53%|█████▎    | 500/939 [2:36:50<2:32:24, 20.83s/i[INFO|trainer.py:3993] 2025-07-15 20:33:32,184 >> Saving model checkpoint to saves/golden/user4/toy_reward/checkpoint-500
[INFO|trainer.py:4007] 2025-07-15 20:33:32,189 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
{'eval_loss': 0.13911020755767822, 'eval_accuracy': 0.915, 'eval_runtime': 127.7755, 'eval_samples_per_second': 7.826, 'eval_steps_per_second': 7.826, 'epoch': 1.6}
[INFO|tokenization_utils_base.py:2356] 2025-07-15 20:33:32,302 >> chat template saved in saves/golden/user4/toy_reward/checkpoint-500/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-15 20:33:32,305 >> tokenizer config file saved in saves/golden/user4/toy_reward/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-15 20:33:32,306 >> Special tokens file saved in saves/golden/user4/toy_reward/checkpoint-500/special_tokens_map.json
[INFO|configuration_utils.py:698] 2025-07-15 20:33:33,630 >> loading configuration file config.json from cache at /mmfs1/gscratch/ark/devinl6/hf_cache/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:770] 2025-07-15 20:33:33,631 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "vocab_size": 128256
}
[INFO|2025-07-15 20:33:33] llamafactory.train.callbacks:143 >> Value head model saved at: saves/golden/user4/toy_reward/checkpoint-500

 64%|██████▍   | 600/939 [3:06:18<1:37:35, 17.27s/it][INFO|trainer.py:4327] 2025-07-15 21:03:00,707 >>
{'loss': 0.0922, 'grad_norm': 2.4223477840423584, 'learning_rate': 5.139401726188208e-06, 'epoch': 1.63}
{'loss': 0.1759, 'grad_norm': 6.723940849304199, 'learning_rate': 4.953527405032723e-06, 'epoch': 1.66}
{'loss': 0.1362, 'grad_norm': 2.0554041862487793, 'learning_rate': 4.767717313182611e-06, 'epoch': 1.69}
{'loss': 0.1254, 'grad_norm': 4.663466453552246, 'learning_rate': 4.582228256894093e-06, 'epoch': 1.73}
{'loss': 0.1417, 'grad_norm': 3.0900464057922363, 'learning_rate': 4.397316598723385e-06, 'epoch': 1.76}
{'loss': 0.104, 'grad_norm': 3.1008188724517822, 'learning_rate': 4.2132379032105695e-06, 'epoch': 1.79}
{'loss': 0.1037, 'grad_norm': 1.564070463180542, 'learning_rate': 4.030246583666437e-06, 'epoch': 1.82}
{'loss': 0.1012, 'grad_norm': 3.13535737991333, 'learning_rate': 3.848595550550401e-06, 'epoch': 1.85}
{'loss': 0.11, 'grad_norm': 2.4275527000427246, 'learning_rate': 3.668535861925509e-06, 'epoch': 1.89}
{'loss': 0.1193, 'grad_norm': 4.933077812194824, 'learning_rate': 3.4903163764736104e-06, 'epoch': 1.92}
***** Running Evaluation *****
[INFO|trainer.py:4329] 2025-07-15 21:03:00,707 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-15 21:03:00,707 >>   Batch size = 1
 75%|███████▍  | 700/939 [3:37:35<1:16:18, 19.16s/it][INFO|trainer.py:4327] 2025-07-15 21:34:17,676 >>
***** Running Evaluation *****                     
{'eval_loss': 0.1388547718524933, 'eval_accuracy': 0.912, 'eval_runtime': 126.6007, 'eval_samples_per_second': 7.899, 'eval_steps_per_second': 7.899, 'epoch': 1.92}
{'loss': 0.1332, 'grad_norm': 4.223021030426025, 'learning_rate': 3.314183409550293e-06, 'epoch': 1.95}
{'loss': 0.1527, 'grad_norm': 2.005488634109497, 'learning_rate': 3.140380392754901e-06, 'epoch': 1.98}
{'loss': 0.1032, 'grad_norm': 4.666478157043457, 'learning_rate': 2.969147537486175e-06, 'epoch': 2.01}
{'loss': 0.1035, 'grad_norm': 2.49334454536438, 'learning_rate': 2.800721502948506e-06, 'epoch': 2.04}
{'loss': 0.159, 'grad_norm': 2.5507824420928955, 'learning_rate': 2.635335069067617e-06, 'epoch': 2.08}
{'loss': 0.1089, 'grad_norm': 1.4469794034957886, 'learning_rate': 2.4732168147677927e-06, 'epoch': 2.11}
{'loss': 0.1192, 'grad_norm': 2.5627822875976562, 'learning_rate': 2.314590802055232e-06, 'epoch': 2.14}
{'loss': 0.0988, 'grad_norm': 2.856339693069458, 'learning_rate': 2.159676266344222e-06, 'epoch': 2.17}
{'loss': 0.0856, 'grad_norm': 2.751251459121704, 'learning_rate': 2.0086873134540626e-06, 'epoch': 2.2}
{'loss': 0.1206, 'grad_norm': 3.33746600151062, 'learning_rate': 1.8618326236955908e-06, 'epoch': 2.24}
[INFO|trainer.py:4329] 2025-07-15 21:34:17,676 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-15 21:34:17,677 >>   Batch size = 1
 85%|████████▌ | 800/939 [4:08:43<37:20, 16.12s/it][INFO|trainer.py:4327] 2025-07-15 22:05:25,957 >>
***** Running Evaluation *****                     
{'eval_loss': 0.139781191945076, 'eval_accuracy': 0.912, 'eval_runtime': 127.0371, 'eval_samples_per_second': 7.872, 'eval_steps_per_second': 7.872, 'epoch': 2.24}
{'loss': 0.1019, 'grad_norm': 4.713781833648682, 'learning_rate': 1.7193151634562071e-06, 'epoch': 2.27}
{'loss': 0.0967, 'grad_norm': 4.311088562011719, 'learning_rate': 1.581331904682089e-06, 'epoch': 2.3}
{'loss': 0.1035, 'grad_norm': 5.7549824714660645, 'learning_rate': 1.4480735526452427e-06, 'epoch': 2.33}
{'loss': 0.1139, 'grad_norm': 2.333933115005493, 'learning_rate': 1.319724282371664e-06, 'epoch': 2.36}
{'loss': 0.0952, 'grad_norm': 5.821939945220947, 'learning_rate': 1.1964614840949002e-06, 'epoch': 2.4}
{'loss': 0.1071, 'grad_norm': 3.4465110301971436, 'learning_rate': 1.078455518086784e-06, 'epoch': 2.43}
{'loss': 0.0833, 'grad_norm': 3.0278148651123047, 'learning_rate': 9.658694792042284e-07, 'epoch': 2.46}
{'loss': 0.0983, 'grad_norm': 5.0931172370910645, 'learning_rate': 8.58858971477457e-07, 'epoch': 2.49}
{'loss': 0.097, 'grad_norm': 6.277773857116699, 'learning_rate': 7.575718930512516e-07, 'epoch': 2.52}
{'loss': 0.1075, 'grad_norm': 3.1432950496673584, 'learning_rate': 6.621482317764105e-07, 'epoch': 2.56}
[INFO|trainer.py:4329] 2025-07-15 22:05:25,957 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-15 22:05:25,957 >>   Batch size = 1
 96%|█████████▌| 900/939 [4:40:21<11:27, 17.62s/it][INFO|trainer.py:4327] 2025-07-15 22:37:04,000 >>
***** Running Evaluation *****                     
{'eval_loss': 0.14192433655261993, 'eval_accuracy': 0.911, 'eval_runtime': 124.9962, 'eval_samples_per_second': 8.0, 'eval_steps_per_second': 8.0, 'epoch': 2.56}
{'loss': 0.0745, 'grad_norm': 1.5001739263534546, 'learning_rate': 5.727198717339511e-07, 'epoch': 2.59}
{'loss': 0.1171, 'grad_norm': 2.9021410942077637, 'learning_rate': 4.894104109594466e-07, 'epoch': 2.62}
{'loss': 0.0708, 'grad_norm': 5.254669666290283, 'learning_rate': 4.123349906194357e-07, 'epoch': 2.65}
{'loss': 0.1066, 'grad_norm': 4.2859578132629395, 'learning_rate': 3.416001358759635e-07, 'epoch': 2.68}
{'loss': 0.1065, 'grad_norm': 4.661489963531494, 'learning_rate': 2.7730360865923954e-07, 'epoch': 2.72}
{'loss': 0.0946, 'grad_norm': 2.1547393798828125, 'learning_rate': 2.1953427255185122e-07, 'epoch': 2.75}
{'loss': 0.1132, 'grad_norm': 8.757837295532227, 'learning_rate': 1.6837196997130434e-07, 'epoch': 2.78}
{'loss': 0.1037, 'grad_norm': 3.931452989578247, 'learning_rate': 1.2388741182062348e-07, 'epoch': 2.81}
{'loss': 0.0888, 'grad_norm': 3.5311570167541504, 'learning_rate': 8.614207975952083e-08, 'epoch': 2.84}
{'loss': 0.0894, 'grad_norm': 4.157740116119385, 'learning_rate': 5.518814123121885e-08, 'epoch': 2.88}
[INFO|trainer.py:4329] 2025-07-15 22:37:04,001 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-15 22:37:04,001 >>   Batch size = 1
100%|██████████| 939/939 [4:53:05<00:00, 15.13s/it][INFO|trainer.py:3993] 2025-07-15 22:49:47,402 >> Saving model checkpoint to saves/golden/user4/toy_reward/checkpoint-939
[INFO|trainer.py:4007] 2025-07-15 22:49:47,405 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
{'eval_loss': 0.1418069452047348, 'eval_accuracy': 0.914, 'eval_runtime': 124.2888, 'eval_samples_per_second': 8.046, 'eval_steps_per_second': 8.046, 'epoch': 2.88}
{'loss': 0.1201, 'grad_norm': 1.9005216360092163, 'learning_rate': 3.10683773623488e-08, 'epoch': 2.91}
{'loss': 0.1218, 'grad_norm': 5.307579040527344, 'learning_rate': 1.3816123835588835e-08, 'epoch': 2.94}
{'loss': 0.1016, 'grad_norm': 2.106980562210083, 'learning_rate': 3.4552248167507576e-09, 'epoch': 2.97}
[INFO|tokenization_utils_base.py:2356] 2025-07-15 22:49:47,411 >> chat template saved in saves/golden/user4/toy_reward/checkpoint-939/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-15 22:49:47,414 >> tokenizer config file saved in saves/golden/user4/toy_reward/checkpoint-939/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-15 22:49:47,415 >> Special tokens file saved in saves/golden/user4/toy_reward/checkpoint-939/special_tokens_map.json
[INFO|configuration_utils.py:698] 2025-07-15 22:49:48,214 >> loading configuration file config.json from cache at /mmfs1/gscratch/ark/devinl6/hf_cache/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:770] 2025-07-15 22:49:48,215 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "vocab_size": 128256
}
[INFO|2025-07-15 22:49:48] llamafactory.train.callbacks:143 >> Value head model saved at: saves/golden/user4/toy_reward/checkpoint-939

[INFO|trainer.py:2676] 2025-07-15 22:49:48,346 >>

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|██████████| 939/939 [4:53:06<00:00, 18.73s/it]
{'train_runtime': 17588.4181, 'train_samples_per_second': 1.706, 'train_steps_per_second': 0.053, 'train_loss': 0.16816769678371782, 'epoch': 3.0}
[INFO|trainer.py:3993] 2025-07-15 22:49:48,349 >> Saving model checkpoint to saves/golden/user4/toy_reward
[INFO|trainer.py:4007] 2025-07-15 22:49:48,353 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|tokenization_utils_base.py:2356] 2025-07-15 22:49:48,357 >> chat template saved in saves/golden/user4/toy_reward/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-15 22:49:48,360 >> tokenizer config file saved in saves/golden/user4/toy_reward/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-15 22:49:48,361 >> Special tokens file saved in saves/golden/user4/toy_reward/special_tokens_map.json
[INFO|configuration_utils.py:698] 2025-07-15 22:49:48,921 >> loading configuration file config.json from cache at /mmfs1/gscratch/ark/devinl6/hf_cache/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:770] 2025-07-15 22:49:48,922 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "vocab_size": 128256
}
[INFO|2025-07-15 22:49:49] llamafactory.train.callbacks:143 >> Value head model saved at: saves/golden/user4/toy_reward
***** train metrics *****
  epoch                    =        3.0
  total_flos               =        0GF
  train_loss               =     0.1682
  train_runtime            = 4:53:08.41
  train_samples_per_second =      1.706
  train_steps_per_second   =      0.053
Figure saved at: saves/golden/user4/toy_reward/training_loss.png
Figure saved at: saves/golden/user4/toy_reward/training_eval_loss.png
Figure saved at: saves/golden/user4/toy_reward/training_eval_accuracy.png

[INFO|trainer.py:4327] 2025-07-15 22:49:50,241 >>
***** Running Evaluation *****
[INFO|trainer.py:4329] 2025-07-15 22:49:50,242 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-15 22:49:50,242 >>   Batch size = 1
100%|██████████| 1000/1000 [02:02<00:00,  8.16it/s]
***** eval metrics *****
  epoch                   =        3.0
  eval_accuracy           =      0.915
  eval_loss               =     0.1429
  eval_runtime            = 0:02:02.90
  eval_samples_per_second =      8.137
  eval_steps_per_second   =      8.137
[INFO|modelcard.py:450] 2025-07-15 22:51:53,145 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.915}]}
