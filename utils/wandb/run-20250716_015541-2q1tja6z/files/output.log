[INFO|integration_utils.py:880] 2025-07-16 01:55:43,364 >> Could not log the number of model parameters in Weights & Biases due to an AttributeError.
 10%|███████▎                                                                 | 10/100 [00:08<00:47,  1.91it/s][INFO|trainer.py:4327] 2025-07-16 01:55:52,126 >>
{'loss': 0.7771, 'grad_norm': 49.699317932128906, 'learning_rate': 9e-06, 'epoch': 1.0}
***** Running Evaluation *****
[INFO|trainer.py:4329] 2025-07-16 01:55:52,127 >>   Num examples = 10
[INFO|trainer.py:4332] 2025-07-16 01:55:52,127 >>   Batch size = 1
 20%|██████████████▌                                                          | 20/100 [00:14<00:39,  2.02it/s][INFO|trainer.py:4327] 2025-07-16 01:55:58,269 >>
***** Running Evaluation *****                                                                                 
{'eval_loss': 0.7988401651382446, 'eval_accuracy': 0.5, 'eval_runtime': 1.473, 'eval_samples_per_second': 6.789, 'eval_steps_per_second': 6.789, 'epoch': 1.0}
{'loss': 0.8469, 'grad_norm': 61.5162467956543, 'learning_rate': 9.755282581475769e-06, 'epoch': 2.0}
[INFO|trainer.py:4329] 2025-07-16 01:55:58,269 >>   Num examples = 10
[INFO|trainer.py:4332] 2025-07-16 01:55:58,270 >>   Batch size = 1
 30%|█████████████████████▉                                                   | 30/100 [00:21<00:34,  2.01it/s][INFO|trainer.py:4327] 2025-07-16 01:56:04,446 >>
***** Running Evaluation *****                                                                                 
{'eval_loss': 0.8405400514602661, 'eval_accuracy': 0.3, 'eval_runtime': 1.4747, 'eval_samples_per_second': 6.781, 'eval_steps_per_second': 6.781, 'epoch': 2.0}
{'loss': 0.657, 'grad_norm': 64.11319732666016, 'learning_rate': 8.94005376803361e-06, 'epoch': 3.0}
[INFO|trainer.py:4329] 2025-07-16 01:56:04,447 >>   Num examples = 10
[INFO|trainer.py:4332] 2025-07-16 01:56:04,447 >>   Batch size = 1
 40%|█████████████████████████████▏                                           | 40/100 [00:27<00:27,  2.17it/s][INFO|trainer.py:4327] 2025-07-16 01:56:10,638 >>
***** Running Evaluation *****                                                                                 
{'eval_loss': 0.9136913418769836, 'eval_accuracy': 0.3, 'eval_runtime': 1.4673, 'eval_samples_per_second': 6.815, 'eval_steps_per_second': 6.815, 'epoch': 3.0}
{'loss': 0.6238, 'grad_norm': 39.28601837158203, 'learning_rate': 7.649596321166024e-06, 'epoch': 4.0}
[INFO|trainer.py:4329] 2025-07-16 01:56:10,638 >>   Num examples = 10
[INFO|trainer.py:4332] 2025-07-16 01:56:10,639 >>   Batch size = 1
 50%|████████████████████████████████████▌                                    | 50/100 [00:33<00:24,  2.06it/s][INFO|trainer.py:4327] 2025-07-16 01:56:16,853 >>
***** Running Evaluation *****                                                                                 
{'eval_loss': 1.06122624874115, 'eval_accuracy': 0.4, 'eval_runtime': 1.483, 'eval_samples_per_second': 6.743, 'eval_steps_per_second': 6.743, 'epoch': 4.0}
{'loss': 0.5107, 'grad_norm': 20.16870880126953, 'learning_rate': 6.039558454088796e-06, 'epoch': 5.0}
[INFO|trainer.py:4329] 2025-07-16 01:56:16,853 >>   Num examples = 10
[INFO|trainer.py:4332] 2025-07-16 01:56:16,853 >>   Batch size = 1
 60%|███████████████████████████████████████████▊                             | 60/100 [00:39<00:17,  2.33it/s][INFO|trainer.py:4327] 2025-07-16 01:56:23,056 >>
***** Running Evaluation *****                                                                                 
{'eval_loss': 1.258697271347046, 'eval_accuracy': 0.4, 'eval_runtime': 1.4687, 'eval_samples_per_second': 6.809, 'eval_steps_per_second': 6.809, 'epoch': 5.0}
{'loss': 0.4359, 'grad_norm': 27.831283569335938, 'learning_rate': 4.304134495199675e-06, 'epoch': 6.0}
[INFO|trainer.py:4329] 2025-07-16 01:56:23,056 >>   Num examples = 10
[INFO|trainer.py:4332] 2025-07-16 01:56:23,056 >>   Batch size = 1
 70%|███████████████████████████████████████████████████                      | 70/100 [00:45<00:13,  2.14it/s][INFO|trainer.py:4327] 2025-07-16 01:56:29,208 >>
***** Running Evaluation *****                                                                                 
{'eval_loss': 1.5190829038619995, 'eval_accuracy': 0.4, 'eval_runtime': 1.4791, 'eval_samples_per_second': 6.761, 'eval_steps_per_second': 6.761, 'epoch': 6.0}
{'loss': 0.3082, 'grad_norm': 20.210800170898438, 'learning_rate': 2.6526421860705474e-06, 'epoch': 7.0}
[INFO|trainer.py:4329] 2025-07-16 01:56:29,208 >>   Num examples = 10
[INFO|trainer.py:4332] 2025-07-16 01:56:29,208 >>   Batch size = 1
 80%|██████████████████████████████████████████████████████████▍              | 80/100 [00:51<00:09,  2.09it/s][INFO|trainer.py:4327] 2025-07-16 01:56:35,308 >>
***** Running Evaluation *****                                                                                 
{'eval_loss': 1.713142991065979, 'eval_accuracy': 0.4, 'eval_runtime': 1.4634, 'eval_samples_per_second': 6.834, 'eval_steps_per_second': 6.834, 'epoch': 7.0}
{'loss': 0.2457, 'grad_norm': 0.49220573902130127, 'learning_rate': 1.2842758726130283e-06, 'epoch': 8.0}
[INFO|trainer.py:4329] 2025-07-16 01:56:35,308 >>   Num examples = 10
[INFO|trainer.py:4332] 2025-07-16 01:56:35,309 >>   Batch size = 1
 90%|█████████████████████████████████████████████████████████████████▋       | 90/100 [00:58<00:04,  2.06it/s][INFO|trainer.py:4327] 2025-07-16 01:56:41,534 >>
***** Running Evaluation *****                                                                                 
{'eval_loss': 1.810066819190979, 'eval_accuracy': 0.4, 'eval_runtime': 1.4843, 'eval_samples_per_second': 6.737, 'eval_steps_per_second': 6.737, 'epoch': 8.0}
{'loss': 0.3981, 'grad_norm': 12.893893241882324, 'learning_rate': 3.6408072716606346e-07, 'epoch': 9.0}
[INFO|trainer.py:4329] 2025-07-16 01:56:41,535 >>   Num examples = 10
[INFO|trainer.py:4332] 2025-07-16 01:56:41,535 >>   Batch size = 1
100%|████████████████████████████████████████████████████████████████████████| 100/100 [01:04<00:00,  1.95it/s][INFO|trainer.py:4327] 2025-07-16 01:56:47,760 >>
***** Running Evaluation *****                                                                                 
{'eval_loss': 1.8724949359893799, 'eval_accuracy': 0.4, 'eval_runtime': 1.5011, 'eval_samples_per_second': 6.662, 'eval_steps_per_second': 6.662, 'epoch': 9.0}
{'loss': 0.1619, 'grad_norm': 1.3423171043395996, 'learning_rate': 3.0458649045211897e-09, 'epoch': 10.0}
[INFO|trainer.py:4329] 2025-07-16 01:56:47,760 >>   Num examples = 10
[INFO|trainer.py:4332] 2025-07-16 01:56:47,760 >>   Batch size = 1
100%|████████████████████████████████████████████████████████████████████████| 100/100 [01:05<00:00,  1.95it/s][INFO|trainer.py:3993] 2025-07-16 01:56:49,242 >> Saving model checkpoint to saves/normal/user1/toy_reward/checkpoint-100
[INFO|trainer.py:4007] 2025-07-16 01:56:49,247 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
{'eval_loss': 1.8779470920562744, 'eval_accuracy': 0.4, 'eval_runtime': 1.4833, 'eval_samples_per_second': 6.742, 'eval_steps_per_second': 6.742, 'epoch': 10.0}
[INFO|tokenization_utils_base.py:2356] 2025-07-16 01:56:49,272 >> chat template saved in saves/normal/user1/toy_reward/checkpoint-100/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-16 01:56:49,276 >> tokenizer config file saved in saves/normal/user1/toy_reward/checkpoint-100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-16 01:56:49,276 >> Special tokens file saved in saves/normal/user1/toy_reward/checkpoint-100/special_tokens_map.json
[INFO|configuration_utils.py:698] 2025-07-16 01:56:50,263 >> loading configuration file config.json from cache at /mmfs1/gscratch/ark/devinl6/hf_cache/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:770] 2025-07-16 01:56:50,264 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "vocab_size": 128256
}
[INFO|2025-07-16 01:56:50] llamafactory.train.callbacks:143 >> Value head model saved at: saves/normal/user1/toy_reward/checkpoint-100

[INFO|trainer.py:2676] 2025-07-16 01:56:50,387 >>

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|████████████████████████████████████████████████████████████████████████| 100/100 [01:07<00:00,  1.49it/s]
{'train_runtime': 68.8766, 'train_samples_per_second': 1.452, 'train_steps_per_second': 1.452, 'train_loss': 0.49652098059654237, 'epoch': 10.0}
[INFO|trainer.py:3993] 2025-07-16 01:56:50,390 >> Saving model checkpoint to saves/normal/user1/toy_reward
[INFO|trainer.py:4007] 2025-07-16 01:56:50,394 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|tokenization_utils_base.py:2356] 2025-07-16 01:56:50,398 >> chat template saved in saves/normal/user1/toy_reward/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-16 01:56:50,401 >> tokenizer config file saved in saves/normal/user1/toy_reward/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-16 01:56:50,402 >> Special tokens file saved in saves/normal/user1/toy_reward/special_tokens_map.json
[INFO|configuration_utils.py:698] 2025-07-16 01:56:50,944 >> loading configuration file config.json from cache at /mmfs1/gscratch/ark/devinl6/hf_cache/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:770] 2025-07-16 01:56:50,946 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "vocab_size": 128256
}
[INFO|2025-07-16 01:56:51] llamafactory.train.callbacks:143 >> Value head model saved at: saves/normal/user1/toy_reward
***** train metrics *****
  epoch                    =       10.0
  total_flos               =        0GF
  train_loss               =     0.4965
  train_runtime            = 0:01:08.87
  train_samples_per_second =      1.452
  train_steps_per_second   =      1.452
Figure saved at: saves/normal/user1/toy_reward/training_loss.png
Figure saved at: saves/normal/user1/toy_reward/training_eval_loss.png
Figure saved at: saves/normal/user1/toy_reward/training_eval_accuracy.png

[INFO|trainer.py:4327] 2025-07-16 01:56:51,920 >>
***** Running Evaluation *****
[INFO|trainer.py:4329] 2025-07-16 01:56:51,920 >>   Num examples = 10
[INFO|trainer.py:4332] 2025-07-16 01:56:51,921 >>   Batch size = 1
100%|██████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  8.46it/s]
***** eval metrics *****
  epoch                   =       10.0
  eval_accuracy           =        0.4
  eval_loss               =     1.8779
  eval_runtime            = 0:00:01.48
  eval_samples_per_second =      6.719
  eval_steps_per_second   =      6.719
[INFO|modelcard.py:450] 2025-07-16 01:56:53,409 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.4}]}
