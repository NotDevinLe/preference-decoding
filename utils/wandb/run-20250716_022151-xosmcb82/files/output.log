[INFO|integration_utils.py:880] 2025-07-16 02:21:52,735 >> Could not log the number of model parameters in Weights & Biases due to an AttributeError.
 11%|█         | 100/939 [29:25<4:50:01, 20.74s/it][INFO|trainer.py:4327] 2025-07-16 02:51:18,266 >>
{'loss': 0.7885, 'grad_norm': 11.066072463989258, 'learning_rate': 9.574468085106384e-07, 'epoch': 0.03}
{'loss': 0.7466, 'grad_norm': 9.412639617919922, 'learning_rate': 2.021276595744681e-06, 'epoch': 0.06}
{'loss': 0.7576, 'grad_norm': 11.406753540039062, 'learning_rate': 3.0851063829787237e-06, 'epoch': 0.1}
{'loss': 0.7589, 'grad_norm': 10.030163764953613, 'learning_rate': 4.148936170212766e-06, 'epoch': 0.13}
{'loss': 0.6993, 'grad_norm': 8.75851058959961, 'learning_rate': 5.212765957446809e-06, 'epoch': 0.16}
{'loss': 0.617, 'grad_norm': 7.0347490310668945, 'learning_rate': 6.276595744680851e-06, 'epoch': 0.19}
{'loss': 0.4594, 'grad_norm': 6.085744380950928, 'learning_rate': 7.340425531914894e-06, 'epoch': 0.22}
{'loss': 0.3109, 'grad_norm': 5.529397964477539, 'learning_rate': 8.404255319148937e-06, 'epoch': 0.26}
{'loss': 0.2254, 'grad_norm': 4.245845794677734, 'learning_rate': 9.46808510638298e-06, 'epoch': 0.29}
{'loss': 0.1765, 'grad_norm': 4.353175163269043, 'learning_rate': 9.999136119166803e-06, 'epoch': 0.32}
***** Running Evaluation *****
[INFO|trainer.py:4329] 2025-07-16 02:51:18,266 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-16 02:51:18,266 >>   Batch size = 1
 21%|██▏       | 200/939 [1:01:41<3:49:11, 18.61s/it][INFO|trainer.py:4327] 2025-07-16 03:23:34,037 >>
***** Running Evaluation *****                     
{'eval_loss': 0.14903779327869415, 'eval_accuracy': 0.92, 'eval_runtime': 127.5929, 'eval_samples_per_second': 7.837, 'eval_steps_per_second': 7.837, 'epoch': 0.32}
{'loss': 0.1824, 'grad_norm': 2.2979156970977783, 'learning_rate': 9.9922268634943e-06, 'epoch': 0.35}
{'loss': 0.1899, 'grad_norm': 2.7667338848114014, 'learning_rate': 9.978417901361958e-06, 'epoch': 0.38}
{'loss': 0.1727, 'grad_norm': 2.5337603092193604, 'learning_rate': 9.95772831799724e-06, 'epoch': 0.42}
{'loss': 0.1142, 'grad_norm': 2.9047458171844482, 'learning_rate': 9.930186708264902e-06, 'epoch': 0.45}
{'loss': 0.171, 'grad_norm': 3.9346718788146973, 'learning_rate': 9.895831137146319e-06, 'epoch': 0.48}
{'loss': 0.0885, 'grad_norm': 3.534518241882324, 'learning_rate': 9.854709087130261e-06, 'epoch': 0.51}
{'loss': 0.1236, 'grad_norm': 4.472332954406738, 'learning_rate': 9.80687739258782e-06, 'epoch': 0.54}
{'loss': 0.1449, 'grad_norm': 2.528594732284546, 'learning_rate': 9.7524021612222e-06, 'epoch': 0.58}
{'loss': 0.1289, 'grad_norm': 3.148174524307251, 'learning_rate': 9.691358682701927e-06, 'epoch': 0.61}
{'loss': 0.1275, 'grad_norm': 3.037484645843506, 'learning_rate': 9.623831324603755e-06, 'epoch': 0.64}
[INFO|trainer.py:4329] 2025-07-16 03:23:34,038 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-16 03:23:34,038 >>   Batch size = 1
 32%|███▏      | 300/939 [1:33:53<3:12:17, 18.06s/it][INFO|trainer.py:4327] 2025-07-16 03:55:46,184 >>
***** Running Evaluation *****                     
{'eval_loss': 0.11109887808561325, 'eval_accuracy': 0.934, 'eval_runtime': 128.3106, 'eval_samples_per_second': 7.794, 'eval_steps_per_second': 7.794, 'epoch': 0.64}
{'loss': 0.1058, 'grad_norm': 0.9573608040809631, 'learning_rate': 9.549913415809084e-06, 'epoch': 0.67}
{'loss': 0.1242, 'grad_norm': 2.338764190673828, 'learning_rate': 9.469707117515068e-06, 'epoch': 0.7}
{'loss': 0.1202, 'grad_norm': 2.3472740650177, 'learning_rate': 9.383323282038632e-06, 'epoch': 0.74}
{'loss': 0.0987, 'grad_norm': 2.7622056007385254, 'learning_rate': 9.29088129960862e-06, 'epoch': 0.77}
{'loss': 0.1135, 'grad_norm': 5.481603145599365, 'learning_rate': 9.192508933357753e-06, 'epoch': 0.8}
{'loss': 0.135, 'grad_norm': 6.124129772186279, 'learning_rate': 9.088342142742493e-06, 'epoch': 0.83}
{'loss': 0.0915, 'grad_norm': 1.0632243156433105, 'learning_rate': 8.978524895634842e-06, 'epoch': 0.86}
{'loss': 0.1096, 'grad_norm': 2.760011672973633, 'learning_rate': 8.86320896934581e-06, 'epoch': 0.9}
{'loss': 0.1121, 'grad_norm': 5.418893337249756, 'learning_rate': 8.742553740855507e-06, 'epoch': 0.93}
{'loss': 0.103, 'grad_norm': 2.3669209480285645, 'learning_rate': 8.616725966539831e-06, 'epoch': 0.96}
[INFO|trainer.py:4329] 2025-07-16 03:55:46,185 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-16 03:55:46,185 >>   Batch size = 1
 43%|████▎     | 400/939 [2:05:51<2:44:38, 18.33s/it][INFO|trainer.py:4327] 2025-07-16 04:27:44,448 >>
***** Running Evaluation *****                     
{'eval_loss': 0.10449182987213135, 'eval_accuracy': 0.94, 'eval_runtime': 128.1791, 'eval_samples_per_second': 7.802, 'eval_steps_per_second': 7.802, 'epoch': 0.96}
{'loss': 0.103, 'grad_norm': 3.5978622436523438, 'learning_rate': 8.485899551698166e-06, 'epoch': 0.99}
{'loss': 0.0751, 'grad_norm': 2.34999680519104, 'learning_rate': 8.350255310200611e-06, 'epoch': 1.02}
{'loss': 0.0825, 'grad_norm': 3.1692399978637695, 'learning_rate': 8.209980714586955e-06, 'epoch': 1.05}
{'loss': 0.1153, 'grad_norm': 2.6100306510925293, 'learning_rate': 8.065269636962765e-06, 'epoch': 1.09}
{'loss': 0.0764, 'grad_norm': 1.6456373929977417, 'learning_rate': 7.916322081050708e-06, 'epoch': 1.12}
{'loss': 0.0904, 'grad_norm': 0.9035489559173584, 'learning_rate': 7.76334390576742e-06, 'epoch': 1.15}
{'loss': 0.0924, 'grad_norm': 2.439088821411133, 'learning_rate': 7.60654654070796e-06, 'epoch': 1.18}
{'loss': 0.1308, 'grad_norm': 3.5109639167785645, 'learning_rate': 7.446146693931111e-06, 'epoch': 1.21}
{'loss': 0.0668, 'grad_norm': 3.1863319873809814, 'learning_rate': 7.282366052449351e-06, 'epoch': 1.25}
{'loss': 0.1265, 'grad_norm': 1.5870752334594727, 'learning_rate': 7.115430975837457e-06, 'epoch': 1.28}
[INFO|trainer.py:4329] 2025-07-16 04:27:44,448 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-16 04:27:44,449 >>   Batch size = 1
 53%|█████▎    | 500/939 [2:37:05<2:01:28, 16.60s/it][INFO|trainer.py:4327] 2025-07-16 04:58:58,010 >>
***** Running Evaluation *****                     
{'eval_loss': 0.1054336279630661, 'eval_accuracy': 0.942, 'eval_runtime': 127.3224, 'eval_samples_per_second': 7.854, 'eval_steps_per_second': 7.854, 'epoch': 1.28}
{'loss': 0.1297, 'grad_norm': 2.6952991485595703, 'learning_rate': 6.945572183383229e-06, 'epoch': 1.31}
{'loss': 0.1049, 'grad_norm': 3.0812573432922363, 'learning_rate': 6.773024435212678e-06, 'epoch': 1.34}
{'loss': 0.0884, 'grad_norm': 1.2231264114379883, 'learning_rate': 6.598026207830428e-06, 'epoch': 1.37}
{'loss': 0.0817, 'grad_norm': 2.9078688621520996, 'learning_rate': 6.4208193645237314e-06, 'epoch': 1.41}
{'loss': 0.091, 'grad_norm': 5.253713607788086, 'learning_rate': 6.241648821085666e-06, 'epoch': 1.44}
{'loss': 0.0772, 'grad_norm': 3.5340452194213867, 'learning_rate': 6.060762207319479e-06, 'epoch': 1.47}
{'loss': 0.0699, 'grad_norm': 0.04250118508934975, 'learning_rate': 5.878409524791931e-06, 'epoch': 1.5}
{'loss': 0.0856, 'grad_norm': 1.7392323017120361, 'learning_rate': 5.694842801308651e-06, 'epoch': 1.53}
{'loss': 0.0865, 'grad_norm': 2.305976152420044, 'learning_rate': 5.510315742589042e-06, 'epoch': 1.57}
{'loss': 0.1262, 'grad_norm': 5.9874467849731445, 'learning_rate': 5.325083381622165e-06, 'epoch': 1.6}
[INFO|trainer.py:4329] 2025-07-16 04:58:58,011 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-16 04:58:58,011 >>   Batch size = 1
 53%|█████▎    | 500/939 [2:39:12<2:01:28, 16.60s/i[INFO|trainer.py:3993] 2025-07-16 05:01:05,740 >> Saving model checkpoint to saves/golden/user6/toy_reward/checkpoint-500
[INFO|trainer.py:4007] 2025-07-16 05:01:05,744 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
{'eval_loss': 0.1014239713549614, 'eval_accuracy': 0.944, 'eval_runtime': 127.7293, 'eval_samples_per_second': 7.829, 'eval_steps_per_second': 7.829, 'epoch': 1.6}
[INFO|tokenization_utils_base.py:2356] 2025-07-16 05:01:05,750 >> chat template saved in saves/golden/user6/toy_reward/checkpoint-500/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-16 05:01:05,753 >> tokenizer config file saved in saves/golden/user6/toy_reward/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-16 05:01:05,753 >> Special tokens file saved in saves/golden/user6/toy_reward/checkpoint-500/special_tokens_map.json
[INFO|configuration_utils.py:698] 2025-07-16 05:01:06,597 >> loading configuration file config.json from cache at /mmfs1/gscratch/ark/devinl6/hf_cache/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:770] 2025-07-16 05:01:06,599 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "vocab_size": 128256
}
[INFO|2025-07-16 05:01:06] llamafactory.train.callbacks:143 >> Value head model saved at: saves/golden/user6/toy_reward/checkpoint-500

 64%|██████▍   | 600/939 [3:09:26<1:47:52, 19.09s/it][INFO|trainer.py:4327] 2025-07-16 05:31:19,704 >>
{'loss': 0.0929, 'grad_norm': 3.2200889587402344, 'learning_rate': 5.139401726188208e-06, 'epoch': 1.63}
{'loss': 0.0798, 'grad_norm': 1.3194068670272827, 'learning_rate': 4.953527405032723e-06, 'epoch': 1.66}
{'loss': 0.1084, 'grad_norm': 2.328845500946045, 'learning_rate': 4.767717313182611e-06, 'epoch': 1.69}
{'loss': 0.1198, 'grad_norm': 2.7688136100769043, 'learning_rate': 4.582228256894093e-06, 'epoch': 1.73}
{'loss': 0.0888, 'grad_norm': 2.1292428970336914, 'learning_rate': 4.397316598723385e-06, 'epoch': 1.76}
{'loss': 0.1094, 'grad_norm': 2.926664352416992, 'learning_rate': 4.2132379032105695e-06, 'epoch': 1.79}
{'loss': 0.0917, 'grad_norm': 3.1943278312683105, 'learning_rate': 4.030246583666437e-06, 'epoch': 1.82}
{'loss': 0.0834, 'grad_norm': 4.672226428985596, 'learning_rate': 3.848595550550401e-06, 'epoch': 1.85}
{'loss': 0.0735, 'grad_norm': 4.742955207824707, 'learning_rate': 3.668535861925509e-06, 'epoch': 1.89}
{'loss': 0.0747, 'grad_norm': 1.8832844495773315, 'learning_rate': 3.4903163764736104e-06, 'epoch': 1.92}
***** Running Evaluation *****
[INFO|trainer.py:4329] 2025-07-16 05:31:19,704 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-16 05:31:19,704 >>   Batch size = 1
 75%|███████▍  | 700/939 [3:40:23<1:18:18, 19.66s/it][INFO|trainer.py:4327] 2025-07-16 06:02:16,510 >>
***** Running Evaluation *****                     
{'eval_loss': 0.11254303902387619, 'eval_accuracy': 0.942, 'eval_runtime': 128.2263, 'eval_samples_per_second': 7.799, 'eval_steps_per_second': 7.799, 'epoch': 1.92}
{'loss': 0.1244, 'grad_norm': 6.244917392730713, 'learning_rate': 3.314183409550293e-06, 'epoch': 1.95}
{'loss': 0.0863, 'grad_norm': 3.0591483116149902, 'learning_rate': 3.140380392754901e-06, 'epoch': 1.98}
{'loss': 0.055, 'grad_norm': 2.46376371383667, 'learning_rate': 2.969147537486175e-06, 'epoch': 2.01}
{'loss': 0.0998, 'grad_norm': 3.524749279022217, 'learning_rate': 2.800721502948506e-06, 'epoch': 2.04}
{'loss': 0.0699, 'grad_norm': 1.4645392894744873, 'learning_rate': 2.635335069067617e-06, 'epoch': 2.08}
{'loss': 0.0898, 'grad_norm': 0.9495016932487488, 'learning_rate': 2.4732168147677927e-06, 'epoch': 2.11}
{'loss': 0.0887, 'grad_norm': 3.21136474609375, 'learning_rate': 2.314590802055232e-06, 'epoch': 2.14}
{'loss': 0.1036, 'grad_norm': 6.7373833656311035, 'learning_rate': 2.159676266344222e-06, 'epoch': 2.17}
{'loss': 0.0906, 'grad_norm': 7.73357629776001, 'learning_rate': 2.0086873134540626e-06, 'epoch': 2.2}
{'loss': 0.0543, 'grad_norm': 4.780338764190674, 'learning_rate': 1.8618326236955908e-06, 'epoch': 2.24}
[INFO|trainer.py:4329] 2025-07-16 06:02:16,510 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-16 06:02:16,510 >>   Batch size = 1
 85%|████████▌ | 800/939 [4:13:11<43:55, 18.96s/it][INFO|trainer.py:4327] 2025-07-16 06:35:03,868 >>
***** Running Evaluation *****                     
{'eval_loss': 0.10852298885583878, 'eval_accuracy': 0.945, 'eval_runtime': 128.3555, 'eval_samples_per_second': 7.791, 'eval_steps_per_second': 7.791, 'epoch': 2.24}
{'loss': 0.0689, 'grad_norm': 2.711491823196411, 'learning_rate': 1.7193151634562071e-06, 'epoch': 2.27}
{'loss': 0.0813, 'grad_norm': 3.4417710304260254, 'learning_rate': 1.581331904682089e-06, 'epoch': 2.3}
{'loss': 0.0615, 'grad_norm': 5.188552379608154, 'learning_rate': 1.4480735526452427e-06, 'epoch': 2.33}
{'loss': 0.0518, 'grad_norm': 5.456982135772705, 'learning_rate': 1.319724282371664e-06, 'epoch': 2.36}
{'loss': 0.0673, 'grad_norm': 1.635561227798462, 'learning_rate': 1.1964614840949002e-06, 'epoch': 2.4}
{'loss': 0.1403, 'grad_norm': 7.222432613372803, 'learning_rate': 1.078455518086784e-06, 'epoch': 2.43}
{'loss': 0.0853, 'grad_norm': 1.5853995084762573, 'learning_rate': 9.658694792042284e-07, 'epoch': 2.46}
{'loss': 0.0629, 'grad_norm': 2.6841559410095215, 'learning_rate': 8.58858971477457e-07, 'epoch': 2.49}
{'loss': 0.0481, 'grad_norm': 3.481236696243286, 'learning_rate': 7.575718930512516e-07, 'epoch': 2.52}
{'loss': 0.072, 'grad_norm': 1.4994313716888428, 'learning_rate': 6.621482317764105e-07, 'epoch': 2.56}
[INFO|trainer.py:4329] 2025-07-16 06:35:03,868 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-16 06:35:03,868 >>   Batch size = 1
 96%|█████████▌| 900/939 [4:44:09<11:35, 17.83s/it][INFO|trainer.py:4327] 2025-07-16 07:06:02,643 >>
***** Running Evaluation *****                     
{'eval_loss': 0.11694306880235672, 'eval_accuracy': 0.944, 'eval_runtime': 129.3029, 'eval_samples_per_second': 7.734, 'eval_steps_per_second': 7.734, 'epoch': 2.56}
{'loss': 0.0422, 'grad_norm': 0.9770522713661194, 'learning_rate': 5.727198717339511e-07, 'epoch': 2.59}
{'loss': 0.0603, 'grad_norm': 3.990823268890381, 'learning_rate': 4.894104109594466e-07, 'epoch': 2.62}
{'loss': 0.1058, 'grad_norm': 4.190594673156738, 'learning_rate': 4.123349906194357e-07, 'epoch': 2.65}
{'loss': 0.1148, 'grad_norm': 4.431479454040527, 'learning_rate': 3.416001358759635e-07, 'epoch': 2.68}
{'loss': 0.0546, 'grad_norm': 3.3306500911712646, 'learning_rate': 2.7730360865923954e-07, 'epoch': 2.72}
{'loss': 0.0782, 'grad_norm': 3.902275800704956, 'learning_rate': 2.1953427255185122e-07, 'epoch': 2.75}
{'loss': 0.0413, 'grad_norm': 1.711706519126892, 'learning_rate': 1.6837196997130434e-07, 'epoch': 2.78}
{'loss': 0.0751, 'grad_norm': 4.7450852394104, 'learning_rate': 1.2388741182062348e-07, 'epoch': 2.81}
{'loss': 0.0812, 'grad_norm': 4.863064289093018, 'learning_rate': 8.614207975952083e-08, 'epoch': 2.84}
{'loss': 0.0756, 'grad_norm': 3.7201457023620605, 'learning_rate': 5.518814123121885e-08, 'epoch': 2.88}
[INFO|trainer.py:4329] 2025-07-16 07:06:02,643 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-16 07:06:02,643 >>   Batch size = 1
100%|██████████| 939/939 [4:58:04<00:00, 16.70s/it][INFO|trainer.py:3993] 2025-07-16 07:19:57,321 >> Saving model checkpoint to saves/golden/user6/toy_reward/checkpoint-939
[INFO|trainer.py:4007] 2025-07-16 07:19:57,324 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
{'eval_loss': 0.11643306165933609, 'eval_accuracy': 0.944, 'eval_runtime': 128.4867, 'eval_samples_per_second': 7.783, 'eval_steps_per_second': 7.783, 'epoch': 2.88}
{'loss': 0.0675, 'grad_norm': 4.182915210723877, 'learning_rate': 3.10683773623488e-08, 'epoch': 2.91}
{'loss': 0.0426, 'grad_norm': 2.029082775115967, 'learning_rate': 1.3816123835588835e-08, 'epoch': 2.94}
{'loss': 0.0558, 'grad_norm': 3.593550443649292, 'learning_rate': 3.4552248167507576e-09, 'epoch': 2.97}
[INFO|tokenization_utils_base.py:2356] 2025-07-16 07:19:57,330 >> chat template saved in saves/golden/user6/toy_reward/checkpoint-939/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-16 07:19:57,334 >> tokenizer config file saved in saves/golden/user6/toy_reward/checkpoint-939/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-16 07:19:57,336 >> Special tokens file saved in saves/golden/user6/toy_reward/checkpoint-939/special_tokens_map.json
[INFO|configuration_utils.py:698] 2025-07-16 07:19:58,120 >> loading configuration file config.json from cache at /mmfs1/gscratch/ark/devinl6/hf_cache/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:770] 2025-07-16 07:19:58,121 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "vocab_size": 128256
}
[INFO|2025-07-16 07:19:58] llamafactory.train.callbacks:143 >> Value head model saved at: saves/golden/user6/toy_reward/checkpoint-939

[INFO|trainer.py:2676] 2025-07-16 07:19:58,255 >>

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|██████████| 939/939 [4:58:05<00:00, 19.05s/it]
{'train_runtime': 17887.3652, 'train_samples_per_second': 1.677, 'train_steps_per_second': 0.052, 'train_loss': 0.14354356304525187, 'epoch': 3.0}
[INFO|trainer.py:3993] 2025-07-16 07:19:58,258 >> Saving model checkpoint to saves/golden/user6/toy_reward
[INFO|trainer.py:4007] 2025-07-16 07:19:58,262 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|tokenization_utils_base.py:2356] 2025-07-16 07:19:58,266 >> chat template saved in saves/golden/user6/toy_reward/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-16 07:19:58,269 >> tokenizer config file saved in saves/golden/user6/toy_reward/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-16 07:19:58,269 >> Special tokens file saved in saves/golden/user6/toy_reward/special_tokens_map.json
[INFO|configuration_utils.py:698] 2025-07-16 07:19:58,804 >> loading configuration file config.json from cache at /mmfs1/gscratch/ark/devinl6/hf_cache/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:770] 2025-07-16 07:19:58,805 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "vocab_size": 128256
}
[INFO|2025-07-16 07:19:58] llamafactory.train.callbacks:143 >> Value head model saved at: saves/golden/user6/toy_reward
***** train metrics *****
  epoch                    =        3.0
  total_flos               =        0GF
  train_loss               =     0.1435
  train_runtime            = 4:58:07.36
  train_samples_per_second =      1.677
  train_steps_per_second   =      0.052
Figure saved at: saves/golden/user6/toy_reward/training_loss.png
Figure saved at: saves/golden/user6/toy_reward/training_eval_loss.png
Figure saved at: saves/golden/user6/toy_reward/training_eval_accuracy.png

[INFO|trainer.py:4327] 2025-07-16 07:19:59,373 >>
***** Running Evaluation *****
[INFO|trainer.py:4329] 2025-07-16 07:19:59,373 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-16 07:19:59,373 >>   Batch size = 1
100%|██████████| 1000/1000 [02:08<00:00,  7.76it/s]
***** eval metrics *****
  epoch                   =        3.0
  eval_accuracy           =      0.947
  eval_loss               =      0.117
  eval_runtime            = 0:02:09.23
  eval_samples_per_second =      7.738
  eval_steps_per_second   =      7.738
[INFO|modelcard.py:450] 2025-07-16 07:22:08,611 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.947}]}
