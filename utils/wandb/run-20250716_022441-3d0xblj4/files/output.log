[INFO|integration_utils.py:880] 2025-07-16 02:24:46,032 >> Could not log the number of model parameters in Weights & Biases due to an AttributeError.
 11%|█         | 100/939 [26:38<3:09:57, 13.58s/it][INFO|trainer.py:4327] 2025-07-16 02:51:24,184 >>
{'loss': 0.76, 'grad_norm': 7.7822136878967285, 'learning_rate': 9.574468085106384e-07, 'epoch': 0.03}
{'loss': 0.7055, 'grad_norm': 7.576355457305908, 'learning_rate': 2.021276595744681e-06, 'epoch': 0.06}
{'loss': 0.7919, 'grad_norm': 8.905710220336914, 'learning_rate': 3.0851063829787237e-06, 'epoch': 0.1}
{'loss': 0.7671, 'grad_norm': 8.413544654846191, 'learning_rate': 4.148936170212766e-06, 'epoch': 0.13}
{'loss': 0.713, 'grad_norm': 7.155584335327148, 'learning_rate': 5.212765957446809e-06, 'epoch': 0.16}
{'loss': 0.7256, 'grad_norm': 7.7894744873046875, 'learning_rate': 6.276595744680851e-06, 'epoch': 0.19}
{'loss': 0.668, 'grad_norm': 7.4440507888793945, 'learning_rate': 7.340425531914894e-06, 'epoch': 0.22}
{'loss': 0.6181, 'grad_norm': 6.436688423156738, 'learning_rate': 8.404255319148937e-06, 'epoch': 0.26}
{'loss': 0.6285, 'grad_norm': 7.353647232055664, 'learning_rate': 9.46808510638298e-06, 'epoch': 0.29}
{'loss': 0.572, 'grad_norm': 6.119380950927734, 'learning_rate': 9.999136119166803e-06, 'epoch': 0.32}
***** Running Evaluation *****
[INFO|trainer.py:4329] 2025-07-16 02:51:24,185 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-16 02:51:24,185 >>   Batch size = 1
 21%|██▏       | 200/939 [56:24<4:20:25, 21.14s/it][INFO|trainer.py:4327] 2025-07-16 03:21:10,562 >>
***** Running Evaluation *****                     
{'eval_loss': 0.5339465141296387, 'eval_accuracy': 0.729, 'eval_runtime': 117.6499, 'eval_samples_per_second': 8.5, 'eval_steps_per_second': 8.5, 'epoch': 0.32}
{'loss': 0.5315, 'grad_norm': 9.778209686279297, 'learning_rate': 9.9922268634943e-06, 'epoch': 0.35}
{'loss': 0.472, 'grad_norm': 6.304722309112549, 'learning_rate': 9.978417901361958e-06, 'epoch': 0.38}
{'loss': 0.5024, 'grad_norm': 8.640384674072266, 'learning_rate': 9.95772831799724e-06, 'epoch': 0.42}
{'loss': 0.4107, 'grad_norm': 6.694428443908691, 'learning_rate': 9.930186708264902e-06, 'epoch': 0.45}
{'loss': 0.44, 'grad_norm': 6.597685813903809, 'learning_rate': 9.895831137146319e-06, 'epoch': 0.48}
{'loss': 0.472, 'grad_norm': 8.059420585632324, 'learning_rate': 9.854709087130261e-06, 'epoch': 0.51}
{'loss': 0.3782, 'grad_norm': 6.325160026550293, 'learning_rate': 9.80687739258782e-06, 'epoch': 0.54}
{'loss': 0.4027, 'grad_norm': 5.998651504516602, 'learning_rate': 9.7524021612222e-06, 'epoch': 0.58}
{'loss': 0.3718, 'grad_norm': 4.706908702850342, 'learning_rate': 9.691358682701927e-06, 'epoch': 0.61}
{'loss': 0.3926, 'grad_norm': 7.9453816413879395, 'learning_rate': 9.623831324603755e-06, 'epoch': 0.64}
[INFO|trainer.py:4329] 2025-07-16 03:21:10,563 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-16 03:21:10,563 >>   Batch size = 1
 32%|███▏      | 300/939 [1:25:21<2:45:57, 15.58s/it][INFO|trainer.py:4327] 2025-07-16 03:50:07,460 >>
***** Running Evaluation *****                     
{'eval_loss': 0.32593804597854614, 'eval_accuracy': 0.818, 'eval_runtime': 118.7142, 'eval_samples_per_second': 8.424, 'eval_steps_per_second': 8.424, 'epoch': 0.64}
{'loss': 0.279, 'grad_norm': 8.268964767456055, 'learning_rate': 9.549913415809084e-06, 'epoch': 0.67}
{'loss': 0.354, 'grad_norm': 7.2884721755981445, 'learning_rate': 9.469707117515068e-06, 'epoch': 0.7}
{'loss': 0.3226, 'grad_norm': 5.891611576080322, 'learning_rate': 9.383323282038632e-06, 'epoch': 0.74}
{'loss': 0.3134, 'grad_norm': 6.395633697509766, 'learning_rate': 9.29088129960862e-06, 'epoch': 0.77}
{'loss': 0.3295, 'grad_norm': 7.640658855438232, 'learning_rate': 9.192508933357753e-06, 'epoch': 0.8}
{'loss': 0.334, 'grad_norm': 8.428304672241211, 'learning_rate': 9.088342142742493e-06, 'epoch': 0.83}
{'loss': 0.3091, 'grad_norm': 6.564143180847168, 'learning_rate': 8.978524895634842e-06, 'epoch': 0.86}
{'loss': 0.3643, 'grad_norm': 4.813232898712158, 'learning_rate': 8.86320896934581e-06, 'epoch': 0.9}
{'loss': 0.3481, 'grad_norm': 5.998440742492676, 'learning_rate': 8.742553740855507e-06, 'epoch': 0.93}
{'loss': 0.2978, 'grad_norm': 7.870730876922607, 'learning_rate': 8.616725966539831e-06, 'epoch': 0.96}
[INFO|trainer.py:4329] 2025-07-16 03:50:07,461 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-16 03:50:07,461 >>   Batch size = 1
 43%|████▎     | 400/939 [1:54:17<2:45:21, 18.41s/it][INFO|trainer.py:4327] 2025-07-16 04:19:03,826 >>
***** Running Evaluation *****                     
{'eval_loss': 0.2815726399421692, 'eval_accuracy': 0.853, 'eval_runtime': 120.2285, 'eval_samples_per_second': 8.317, 'eval_steps_per_second': 8.317, 'epoch': 0.96}
{'loss': 0.3107, 'grad_norm': 5.276959419250488, 'learning_rate': 8.485899551698166e-06, 'epoch': 0.99}
{'loss': 0.258, 'grad_norm': 6.939204692840576, 'learning_rate': 8.350255310200611e-06, 'epoch': 1.02}
{'loss': 0.2541, 'grad_norm': 9.43136978149414, 'learning_rate': 8.209980714586955e-06, 'epoch': 1.05}
{'loss': 0.3292, 'grad_norm': 9.923979759216309, 'learning_rate': 8.065269636962765e-06, 'epoch': 1.09}
{'loss': 0.2261, 'grad_norm': 5.418953895568848, 'learning_rate': 7.916322081050708e-06, 'epoch': 1.12}
{'loss': 0.296, 'grad_norm': 6.112527370452881, 'learning_rate': 7.76334390576742e-06, 'epoch': 1.15}
{'loss': 0.2404, 'grad_norm': 8.561344146728516, 'learning_rate': 7.60654654070796e-06, 'epoch': 1.18}
{'loss': 0.2498, 'grad_norm': 6.619996547698975, 'learning_rate': 7.446146693931111e-06, 'epoch': 1.21}
{'loss': 0.2848, 'grad_norm': 5.590364456176758, 'learning_rate': 7.282366052449351e-06, 'epoch': 1.25}
{'loss': 0.2394, 'grad_norm': 6.595285892486572, 'learning_rate': 7.115430975837457e-06, 'epoch': 1.28}
[INFO|trainer.py:4329] 2025-07-16 04:19:03,826 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-16 04:19:03,826 >>   Batch size = 1
 53%|█████▎    | 500/939 [2:23:54<1:55:53, 15.84s/it][INFO|trainer.py:4327] 2025-07-16 04:48:40,120 >>
***** Running Evaluation *****                     
{'eval_loss': 0.2704956829547882, 'eval_accuracy': 0.86, 'eval_runtime': 116.1208, 'eval_samples_per_second': 8.612, 'eval_steps_per_second': 8.612, 'epoch': 1.28}
{'loss': 0.2824, 'grad_norm': 5.735518932342529, 'learning_rate': 6.945572183383229e-06, 'epoch': 1.31}
{'loss': 0.257, 'grad_norm': 4.785811901092529, 'learning_rate': 6.773024435212678e-06, 'epoch': 1.34}
{'loss': 0.2732, 'grad_norm': 7.555267810821533, 'learning_rate': 6.598026207830428e-06, 'epoch': 1.37}
{'loss': 0.2627, 'grad_norm': 5.722927093505859, 'learning_rate': 6.4208193645237314e-06, 'epoch': 1.41}
{'loss': 0.2782, 'grad_norm': 5.67594575881958, 'learning_rate': 6.241648821085666e-06, 'epoch': 1.44}
{'loss': 0.2808, 'grad_norm': 8.054518699645996, 'learning_rate': 6.060762207319479e-06, 'epoch': 1.47}
{'loss': 0.2685, 'grad_norm': 5.579382419586182, 'learning_rate': 5.878409524791931e-06, 'epoch': 1.5}
{'loss': 0.2828, 'grad_norm': 9.555843353271484, 'learning_rate': 5.694842801308651e-06, 'epoch': 1.53}
{'loss': 0.2304, 'grad_norm': 6.3182573318481445, 'learning_rate': 5.510315742589042e-06, 'epoch': 1.57}
{'loss': 0.2264, 'grad_norm': 4.946848392486572, 'learning_rate': 5.325083381622165e-06, 'epoch': 1.6}
[INFO|trainer.py:4329] 2025-07-16 04:48:40,121 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-16 04:48:40,121 >>   Batch size = 1
 53%|█████▎    | 500/939 [2:25:50<1:55:53, 15.84s/i[INFO|trainer.py:3993] 2025-07-16 04:50:36,370 >> Saving model checkpoint to saves/golden/user7/toy_reward/checkpoint-500
[INFO|trainer.py:4007] 2025-07-16 04:50:36,375 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
{'eval_loss': 0.2602611780166626, 'eval_accuracy': 0.864, 'eval_runtime': 116.2507, 'eval_samples_per_second': 8.602, 'eval_steps_per_second': 8.602, 'epoch': 1.6}
[INFO|tokenization_utils_base.py:2356] 2025-07-16 04:50:36,458 >> chat template saved in saves/golden/user7/toy_reward/checkpoint-500/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-16 04:50:36,462 >> tokenizer config file saved in saves/golden/user7/toy_reward/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-16 04:50:36,463 >> Special tokens file saved in saves/golden/user7/toy_reward/checkpoint-500/special_tokens_map.json
[INFO|configuration_utils.py:698] 2025-07-16 04:50:37,794 >> loading configuration file config.json from cache at /mmfs1/gscratch/ark/devinl6/hf_cache/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:770] 2025-07-16 04:50:37,795 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "vocab_size": 128256
}
[INFO|2025-07-16 04:50:37] llamafactory.train.callbacks:143 >> Value head model saved at: saves/golden/user7/toy_reward/checkpoint-500

 64%|██████▍   | 600/939 [2:52:23<1:14:13, 13.14s/it][INFO|trainer.py:4327] 2025-07-16 05:17:09,268 >>
{'loss': 0.3033, 'grad_norm': 9.246684074401855, 'learning_rate': 5.139401726188208e-06, 'epoch': 1.63}
{'loss': 0.2886, 'grad_norm': 7.063270568847656, 'learning_rate': 4.953527405032723e-06, 'epoch': 1.66}
{'loss': 0.2781, 'grad_norm': 6.43516731262207, 'learning_rate': 4.767717313182611e-06, 'epoch': 1.69}
{'loss': 0.3543, 'grad_norm': 9.29816722869873, 'learning_rate': 4.582228256894093e-06, 'epoch': 1.73}
{'loss': 0.3052, 'grad_norm': 6.584821701049805, 'learning_rate': 4.397316598723385e-06, 'epoch': 1.76}
{'loss': 0.2359, 'grad_norm': 4.642167091369629, 'learning_rate': 4.2132379032105695e-06, 'epoch': 1.79}
{'loss': 0.2668, 'grad_norm': 4.2951483726501465, 'learning_rate': 4.030246583666437e-06, 'epoch': 1.82}
{'loss': 0.2509, 'grad_norm': 5.864295482635498, 'learning_rate': 3.848595550550401e-06, 'epoch': 1.85}
{'loss': 0.2842, 'grad_norm': 6.309165000915527, 'learning_rate': 3.668535861925509e-06, 'epoch': 1.89}
{'loss': 0.2282, 'grad_norm': 6.325769901275635, 'learning_rate': 3.4903163764736104e-06, 'epoch': 1.92}
***** Running Evaluation *****
[INFO|trainer.py:4329] 2025-07-16 05:17:09,268 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-16 05:17:09,269 >>   Batch size = 1
 75%|███████▍  | 700/939 [3:20:46<1:21:46, 20.53s/it][INFO|trainer.py:4327] 2025-07-16 05:45:32,544 >>
***** Running Evaluation *****                     
{'eval_loss': 0.2536340355873108, 'eval_accuracy': 0.866, 'eval_runtime': 115.1243, 'eval_samples_per_second': 8.686, 'eval_steps_per_second': 8.686, 'epoch': 1.92}
{'loss': 0.2977, 'grad_norm': 5.873660087585449, 'learning_rate': 3.314183409550293e-06, 'epoch': 1.95}
{'loss': 0.2121, 'grad_norm': 4.454904079437256, 'learning_rate': 3.140380392754901e-06, 'epoch': 1.98}
{'loss': 0.2255, 'grad_norm': 9.202815055847168, 'learning_rate': 2.969147537486175e-06, 'epoch': 2.01}
{'loss': 0.2343, 'grad_norm': 5.81751823425293, 'learning_rate': 2.800721502948506e-06, 'epoch': 2.04}
{'loss': 0.192, 'grad_norm': 6.349497318267822, 'learning_rate': 2.635335069067617e-06, 'epoch': 2.08}
{'loss': 0.1834, 'grad_norm': 3.9353365898132324, 'learning_rate': 2.4732168147677927e-06, 'epoch': 2.11}
{'loss': 0.2081, 'grad_norm': 5.34848165512085, 'learning_rate': 2.314590802055232e-06, 'epoch': 2.14}
{'loss': 0.2375, 'grad_norm': 5.457569599151611, 'learning_rate': 2.159676266344222e-06, 'epoch': 2.17}
{'loss': 0.2125, 'grad_norm': 6.918572425842285, 'learning_rate': 2.0086873134540626e-06, 'epoch': 2.2}
{'loss': 0.1632, 'grad_norm': 5.788376331329346, 'learning_rate': 1.8618326236955908e-06, 'epoch': 2.24}
[INFO|trainer.py:4329] 2025-07-16 05:45:32,545 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-16 05:45:32,545 >>   Batch size = 1
 85%|████████▌ | 800/939 [3:50:03<38:16, 16.52s/it][INFO|trainer.py:4327] 2025-07-16 06:14:49,528 >>
***** Running Evaluation *****                     
{'eval_loss': 0.25175827741622925, 'eval_accuracy': 0.869, 'eval_runtime': 116.3162, 'eval_samples_per_second': 8.597, 'eval_steps_per_second': 8.597, 'epoch': 2.24}
{'loss': 0.2244, 'grad_norm': 7.791849136352539, 'learning_rate': 1.7193151634562071e-06, 'epoch': 2.27}
{'loss': 0.2455, 'grad_norm': 7.8278632164001465, 'learning_rate': 1.581331904682089e-06, 'epoch': 2.3}
{'loss': 0.1853, 'grad_norm': 3.7999753952026367, 'learning_rate': 1.4480735526452427e-06, 'epoch': 2.33}
{'loss': 0.2312, 'grad_norm': 4.641890525817871, 'learning_rate': 1.319724282371664e-06, 'epoch': 2.36}
{'loss': 0.1861, 'grad_norm': 8.293058395385742, 'learning_rate': 1.1964614840949002e-06, 'epoch': 2.4}
{'loss': 0.2029, 'grad_norm': 8.087185859680176, 'learning_rate': 1.078455518086784e-06, 'epoch': 2.43}
{'loss': 0.2186, 'grad_norm': 6.039654731750488, 'learning_rate': 9.658694792042284e-07, 'epoch': 2.46}
{'loss': 0.208, 'grad_norm': 6.83051061630249, 'learning_rate': 8.58858971477457e-07, 'epoch': 2.49}
{'loss': 0.2094, 'grad_norm': 6.4274983406066895, 'learning_rate': 7.575718930512516e-07, 'epoch': 2.52}
{'loss': 0.2177, 'grad_norm': 2.4611616134643555, 'learning_rate': 6.621482317764105e-07, 'epoch': 2.56}
[INFO|trainer.py:4329] 2025-07-16 06:14:49,529 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-16 06:14:49,529 >>   Batch size = 1
 96%|█████████▌| 900/939 [4:19:08<11:33, 17.78s/it][INFO|trainer.py:4327] 2025-07-16 06:43:54,633 >>
***** Running Evaluation *****                     
{'eval_loss': 0.24867020547389984, 'eval_accuracy': 0.871, 'eval_runtime': 116.2766, 'eval_samples_per_second': 8.6, 'eval_steps_per_second': 8.6, 'epoch': 2.56}
{'loss': 0.2043, 'grad_norm': 7.581936359405518, 'learning_rate': 5.727198717339511e-07, 'epoch': 2.59}
{'loss': 0.2123, 'grad_norm': 6.30046272277832, 'learning_rate': 4.894104109594466e-07, 'epoch': 2.62}
{'loss': 0.2578, 'grad_norm': 6.935555934906006, 'learning_rate': 4.123349906194357e-07, 'epoch': 2.65}
{'loss': 0.2331, 'grad_norm': 4.108452796936035, 'learning_rate': 3.416001358759635e-07, 'epoch': 2.68}
{'loss': 0.2563, 'grad_norm': 4.817877292633057, 'learning_rate': 2.7730360865923954e-07, 'epoch': 2.72}
{'loss': 0.1942, 'grad_norm': 5.464683532714844, 'learning_rate': 2.1953427255185122e-07, 'epoch': 2.75}
{'loss': 0.1841, 'grad_norm': 2.972492218017578, 'learning_rate': 1.6837196997130434e-07, 'epoch': 2.78}
{'loss': 0.1908, 'grad_norm': 3.7387707233428955, 'learning_rate': 1.2388741182062348e-07, 'epoch': 2.81}
{'loss': 0.2194, 'grad_norm': 6.365325927734375, 'learning_rate': 8.614207975952083e-08, 'epoch': 2.84}
{'loss': 0.2135, 'grad_norm': 6.3897600173950195, 'learning_rate': 5.518814123121885e-08, 'epoch': 2.88}
[INFO|trainer.py:4329] 2025-07-16 06:43:54,633 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-16 06:43:54,633 >>   Batch size = 1
100%|██████████| 939/939 [4:32:04<00:00, 12.70s/it][INFO|trainer.py:3993] 2025-07-16 06:56:50,931 >> Saving model checkpoint to saves/golden/user7/toy_reward/checkpoint-939
[INFO|trainer.py:4007] 2025-07-16 06:56:50,936 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
{'eval_loss': 0.24917785823345184, 'eval_accuracy': 0.869, 'eval_runtime': 116.6499, 'eval_samples_per_second': 8.573, 'eval_steps_per_second': 8.573, 'epoch': 2.88}
{'loss': 0.2713, 'grad_norm': 6.760879039764404, 'learning_rate': 3.10683773623488e-08, 'epoch': 2.91}
{'loss': 0.2267, 'grad_norm': 4.010684490203857, 'learning_rate': 1.3816123835588835e-08, 'epoch': 2.94}
{'loss': 0.1954, 'grad_norm': 10.103672981262207, 'learning_rate': 3.4552248167507576e-09, 'epoch': 2.97}
[INFO|tokenization_utils_base.py:2356] 2025-07-16 06:56:50,942 >> chat template saved in saves/golden/user7/toy_reward/checkpoint-939/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-16 06:56:50,945 >> tokenizer config file saved in saves/golden/user7/toy_reward/checkpoint-939/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-16 06:56:50,946 >> Special tokens file saved in saves/golden/user7/toy_reward/checkpoint-939/special_tokens_map.json
[INFO|configuration_utils.py:698] 2025-07-16 06:56:51,840 >> loading configuration file config.json from cache at /mmfs1/gscratch/ark/devinl6/hf_cache/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:770] 2025-07-16 06:56:51,841 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "vocab_size": 128256
}
[INFO|2025-07-16 06:56:51] llamafactory.train.callbacks:143 >> Value head model saved at: saves/golden/user7/toy_reward/checkpoint-939

[INFO|trainer.py:2676] 2025-07-16 06:56:51,980 >>

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|██████████| 939/939 [4:32:05<00:00, 17.39s/it]
{'train_runtime': 16330.6109, 'train_samples_per_second': 1.837, 'train_steps_per_second': 0.057, 'train_loss': 0.3194774640634799, 'epoch': 3.0}
[INFO|trainer.py:3993] 2025-07-16 06:56:51,983 >> Saving model checkpoint to saves/golden/user7/toy_reward
[INFO|trainer.py:4007] 2025-07-16 06:56:51,987 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|tokenization_utils_base.py:2356] 2025-07-16 06:56:51,992 >> chat template saved in saves/golden/user7/toy_reward/chat_template.jinja
[INFO|tokenization_utils_base.py:2525] 2025-07-16 06:56:51,995 >> tokenizer config file saved in saves/golden/user7/toy_reward/tokenizer_config.json
[INFO|tokenization_utils_base.py:2534] 2025-07-16 06:56:51,996 >> Special tokens file saved in saves/golden/user7/toy_reward/special_tokens_map.json
[INFO|configuration_utils.py:698] 2025-07-16 06:56:52,606 >> loading configuration file config.json from cache at /mmfs1/gscratch/ark/devinl6/hf_cache/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/config.json
[INFO|configuration_utils.py:770] 2025-07-16 06:56:52,607 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "vocab_size": 128256
}
[INFO|2025-07-16 06:56:52] llamafactory.train.callbacks:143 >> Value head model saved at: saves/golden/user7/toy_reward
***** train metrics *****
  epoch                    =        3.0
  total_flos               =        0GF
  train_loss               =     0.3195
  train_runtime            = 4:32:10.61
  train_samples_per_second =      1.837
  train_steps_per_second   =      0.057
Figure saved at: saves/golden/user7/toy_reward/training_loss.png
Figure saved at: saves/golden/user7/toy_reward/training_eval_loss.png
Figure saved at: saves/golden/user7/toy_reward/training_eval_accuracy.png

[INFO|trainer.py:4327] 2025-07-16 06:56:53,644 >>
***** Running Evaluation *****
[INFO|trainer.py:4329] 2025-07-16 06:56:53,645 >>   Num examples = 1000
[INFO|trainer.py:4332] 2025-07-16 06:56:53,645 >>   Batch size = 1
100%|██████████| 1000/1000 [01:55<00:00,  8.62it/s]
***** eval metrics *****
  epoch                   =        3.0
  eval_accuracy           =      0.869
  eval_loss               =     0.2488
  eval_runtime            = 0:01:56.45
  eval_samples_per_second =      8.587
  eval_steps_per_second   =      8.587
[INFO|modelcard.py:450] 2025-07-16 06:58:50,097 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.869}]}
